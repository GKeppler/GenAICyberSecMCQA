{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Inference comparison on A100 40GB: llama.ccp works best!\n",
    "\n",
    "This notbook explorer the different frameworks for running inference of llms with GPU support.  \n",
    "TLDR: GGUF+llama.ccp fastest with GPU support, AWQ, then GPTQ slower. Errors on vLLM due to torch fuckup. ctransformer error. classic model without quantization leads to out of memory.\n",
    "\n",
    "### initial setup transformers based\n",
    "python3.9 -m pip install virtualenv  \n",
    "python3.9 -m virtualenv .venv  \n",
    "source .venv/bin/activate  \n",
    "pip3 install torch torchvision torchaudio  \n",
    "\n",
    ">>> import torch  \n",
    ">>> torch.cuda.is_available()  \n",
    "True  \n",
    "\n",
    "pip3 install transformers  \n",
    "pip3 install autoawq  \n",
    "pip install optimum \n",
    "pip install auto-gptq  \n",
    "\n",
    "### initial setup ctransformers & llama.ccp based\n",
    "\n",
    "python3.9 -m virtualenv .venvctrans  \n",
    "source .venvctrans/bin/activate  \n",
    "pip install ctransformers  \n",
    "module load devel/cuda/12.0  \n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python  \n",
    "pip install langchain  \n",
    "\n",
    "### Frameworks: vLLM, llama.ccp (python), ctransformer, huggingface transformers with Model Formats: GGML, GGUF, AWQ, GPTQ, HF, bin\n",
    "\n",
    "Those are the frameworks built for running the llms and the availabe model formats.  \n",
    "\n",
    "\n",
    "transformers with hf-format:  \n",
    "dmesg -T| grep -E -i -B100 'killed process': Memory cgroup out of memory: Killed process 420230 (python) total-vm:13048064kB, anon-rss:6184252kB, file-rss:14572kB, shmem-rss:4kB, UID:248630 pgtables:20528kB oom_score_adj:0\n",
    "\n",
    "\n",
    "AutoAWQ + AWQ: GPU inf. works  \n",
    "Transformers + AWQ: error: dont use  \n",
    "\n",
    "vllm install kills torch installation:  \n",
    "ImportError: libcupti.so.11.7: cannot open shared object file: No such file or directory  \n",
    "import torch gets error. Fresh env with \"pip install vllm\" still error. Maybe use NVIDIA PyTorch Docker image as recommended from vllm website  \n",
    "\n",
    "GPTQ + AutoGPTQ + optimum + transformers: GPU inf. works  \n",
    "\n",
    "GGUF + ctransformers: with cuda12.0 loaded: OSError: /lib64/libm.so.6: version `GLIBC_2.29' not found  \n",
    "\n",
    "GGUF + llama.ccp.python: works with GPU support:  module load devel/cuda/12.0 needs to be loaded in every new console  \n",
    "\n",
    "### execution time llama2 7B chat: \"Tell me about AI\" from before import to finish, model already downloaded.\n",
    "!!!!The output is not the same for every prompt, so no exact comparison!!!!\n",
    "Execution time GPTQ: 42.51317858695984 seconds  \n",
    "Execution time AWQ: 22.660392999649048 seconds  \n",
    "Execution time vllm+AWQ: 14.192400693893433 seconds\n",
    "Execution time llama.ccp: 8.327299356460571 seconds  \n",
    "### TODO\n",
    "test vLLM in docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iai/sb7059/git/llm_test/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Huggingface tranformers + pytorch bin\n",
    "# no quantization e.g. https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1/tree/main\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"Xwin-LM/Xwin-LM-7B-V0.1\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Xwin-LM/Xwin-LM-7B-V0.1\")\n",
    "(\n",
    "    prompt := \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "            \"USER: Hello, can you help me? \"\n",
    "            \"ASSISTANT:\"\n",
    ")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "samples = model.generate(**inputs, max_new_tokens=4096, temperature=0.7)\n",
    "output = tokenizer.decode(samples[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(output) \n",
    "# Of course! I'm here to help. Please feel free to ask your question or describe the issue you're having, and I'll do my best to assist you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:44:20 llm_engine.py:72] Initializing an LLM engine with config: model='Xwin-LM/Xwin-LM-7B-V0.1', tokenizer='Xwin-LM/Xwin-LM-7B-V0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    }
   ],
   "source": [
    "# vllm + pytorch bin\n",
    "# no quantization e.g https://huggingface.co/Xwin-LM/Xwin-LM-7B-V0.1/tree/main\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "(\n",
    "    prompt := \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "            \"The assistant gives helpful, detailed, and polite answers to the user's questions. \"\n",
    "            \"USER: Hello, can you help me? \"\n",
    "            \"ASSISTANT:\"\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.7, max_tokens=4096)\n",
    "llm = LLM(model=\"Xwin-LM/Xwin-LM-7B-V0.1\")\n",
    "outputs = llm.generate([prompt,], sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWQ\n",
    "The newest quantization method: according to anthropic AWQ achieves 1.45x speedup over GPTQ and is 1.85x faster than cuBLAS FP16 implementation  \n",
    "Code ist from: https://huggingface.co/TheBloke/Llama-2-7b-Chat-AWQ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 127995.19it/s]\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:02<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "Output:  <s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI technology can be used in a variety of applications, including natural language processing, image and speech recognition, and predictive analytics.\n",
      "There are several types of AI, including:\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as playing chess or recognizing faces.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can, and is still a topic of ongoing research and development.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds, and is considered a potential future development.\n",
      "4. Artificial General Intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can, and is considered a long-term goal of AI research.\n",
      "5. Cognitive computing: This type of AI is designed to mimic the way the human brain works, and is used in applications such as image and speech recognition.\n",
      "6. Machine learning: This type of AI is a subset of AI that involves training algorithms to learn from data, and is used in applications such as recommendation systems and fraud detection.\n",
      "7. Robotics: This type of AI is used in applications such as manufacturing and transportation, and involves the use of robots to perform tasks that typically require human intelligence.\n",
      "8. Natural language processing (NLP): This type of AI is used in applications such as chatbots and voice assistants, and involves the use of algorithms to analyze and generate human language.\n",
      "9. Computer vision: This type of AI is used in applications such as image recognition and autonomous vehicles, and involves the use of algorithms to analyze and interpret visual data.\n",
      "10. Predictive analytics: This type of AI is used in applications such as fraud detection and medical diagnosis, and involves the use of algorithms to analyze and make predictions about future events.\n",
      "It is important to note that AI is a rapidly evolving field, and the terminology and definitions may change over time.\n",
      "I hope this helps! Let me know if you have any other questions.</s>\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(\"Output: \", tokenizer.decode(generation_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 110153.44it/s]\n",
      "Replacing layers...: 100%|██████████| 32/32 [00:03<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Pipeline:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAWQForCausalLM' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m prompt_template\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39m[INST] <<SYS>>\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt know the answer to a question, please don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt share false information.\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m<</SYS>>\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m[/INST]\u001b[39m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*** Pipeline:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     23\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     24\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     25\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     26\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     28\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.1\u001b[39;49m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(pipe(prompt_template)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/git/llm_test/.venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:844\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m    834\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    835\u001b[0m         model,\n\u001b[1;32m    836\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    842\u001b[0m     )\n\u001b[0;32m--> 844\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mconfig\n\u001b[1;32m    845\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    846\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING \u001b[39mor\u001b[39;00m model_config\u001b[39m.\u001b[39mtokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/llm_test/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaAWQForCausalLM' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-AWQ\"\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-17 16:47:04 llm_engine.py:72] Initializing an LLM engine with config: model='TheBloke/Llama-2-7b-Chat-AWQ', tokenizer='TheBloke/Llama-2-7b-Chat-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=awq, seed=0)\n",
      "INFO 10-17 16:47:04 tokenizer.py:31] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.\n",
      "INFO 10-17 16:47:14 llm_engine.py:207] # GPU blocks: 3858, # CPU blocks: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\nTell me about AI[/INST]\\n\\n\", Generated text: \"I'm glad you're interested in learning about AI! AI\"\n",
      "Execution time vLLM+AWQ: 15.24031686782837 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "llm = LLM(model=\"TheBloke/Llama-2-7b-Chat-AWQ\", quantization=\"awq\")\n",
    "\n",
    "outputs = llm.generate(prompt_template, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    \n",
    "et = time.time()\n",
    "\n",
    "# get the execution time\n",
    "elapsed_time = et - st\n",
    "print('Execution time vLLM+AWQ:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ\n",
    "https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ\n",
    "ImportError: Loading a GPTQ quantized model requires optimum (`pip install optimum`) and auto-gptq library (`pip install auto-gptq`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iai/sb7059/git/llm_test/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-64g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GGUF\n",
    "https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "#wget download model link\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7b-Chat-GGUF\", model_file=\"/home/iai/sb7059/git/llm_test/llama-2-7b-chat.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/iai/sb7059/git/llm_test/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    callback_manager=callback_manager, \n",
    "    verbose=True, # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(llm(prompt_template))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
