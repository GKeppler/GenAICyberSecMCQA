{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import pandas as pd\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain_community.embeddings.sentence_transformer import (SentenceTransformerEmbeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You are an assistant tasked with summarizing tables and text. \n",
    "Give a concise summary of the given table and text.\n",
    "{content_of_pdf} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOOK_PDF = '/home/iai/sb7059/git/llm_test/data/Book/industrial-cybersecurity-efficiently-monitor-the-cybersecurity-posture-of-your-ics-environment_compress.pdf'\n",
    "PATH = '/home/iai/sb7059/git/llm_test/data/Book/Images'\n",
    "#BOOK_PDF = '/home/iai/sb7059/git/llm_test/data/Book/fdgth-06-1321485.pdf'\n",
    "BOOK_PDF = '/home/iai/sb7059/git/llm_test/data/Book/smeggitt.pdf'\n",
    "WORKSPACE_DIC = \"/hkfs/work/workspace_haic/scratch/sb7059-llm_models_jeremy\"\n",
    "\n",
    "MODEL_PATH = { #\"Mixtral-8x-7b\": WORKSPACE_DIC + \"/Mixtral/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "               #\"Phi-2\": WORKSPACE_DIC + \"/Phi/Phi2/phi-2.Q4_K_M.gguf\",\n",
    "               #\"Llama2-70b\": WORKSPACE_DIC + \"/Llama/Llama2/llama-2-70b.Q5_K_M.gguf\",\n",
    "                \"Phi-3-medium-128k\": WORKSPACE_DIC + \"/Phi/Phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "               #\"LLama-3-70b\": WORKSPACE_DIC + \"/Llama/LLama3/Meta-Llama-3-70B-Instruct-v2.Q4_K_M.gguf\",\n",
    "               #\"Mixtral-8x22b\": WORKSPACE_DIC + \"/Mixtral/Mixtral-8x22b-Instruct\",\n",
    "               #\"Mixtral-8x-22b\": WORKSPACE_DIC + \"/Mixtral/Mixtral-8x22B-Instruct-v0.1.Q4_K_M-00001-of-00002.gguf\",\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document object\n",
    "doc = fitz.open(BOOK_PDF)\n",
    "\n",
    "content_of_pdf = \"\"\n",
    "\n",
    "#Iterate over all pages in the documents\n",
    "#for i in range(doc.page_count):\n",
    "for i in range(0, 3):\n",
    "  page = doc.load_page(i)\n",
    "\n",
    "  # read text and print it\n",
    "  text = page.get_text()\n",
    "\n",
    "  #Add the text to a string to be used in the prompt\n",
    "  content_of_pdf = content_of_pdf + text\n",
    "\n",
    "\n",
    "  # Extract all the images on the page and save the images\n",
    "  for i in page.get_images(full=True):\n",
    "    xref = i[0]\n",
    "    base_image = doc.extract_image(xref)\n",
    "    image_bytes = base_image[\"image\"]\n",
    "    image = fitz.Pixmap(doc, xref)\n",
    "    with open(f'{PATH}/image_{xref}.png', 'wb') as f:\n",
    "      f.write(image.tobytes())\n",
    "\n",
    "  # Extract all the tables on the page and save the tables\n",
    "  tabs = page.find_tables()  # detect the tables\n",
    "  for i,tab in enumerate(tabs):  # iterate over all tables\n",
    "      print(f\"Table {i} column names: {tab.header.names}, external: {tab.header.external}\")\n",
    "      tab = tabs[i]\n",
    "      df = tab.to_pandas()\n",
    "      #Add the table to a string to be used in the prompt\n",
    "      content_of_pdf += df.to_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MEDJACK Attacks:   The Scariest Part of the Hospital                                            Sinclair Meggitt  Comp 116  Tufts University  December 12th, 2018            Table of Contents      Abstract 2  Introduction 2  To the Community 2  Medical Device Vulnerabilities 3  I. The Internet of Things 3  II. A Black Hole 3  MEDJACK Attack 3  I. History 3  II. Anatomy of Attack 4  III. Malware 4  MEDJACK Defense 5  I. Remediation 5  II. Recommendations and Best Practices 5  Conclusion 6  Works Cited 7                                  Abstract  As of 2015, the healthcare industry became the most attacked industry, experiencing 32.7% of all  known breaches nationwide. (TrapX, 2015) The increased targeting is due to three main reasons:  patient records are extremely valuable. the healthcare industry is notoriously slow to evolve  making it an easy target, and hospitals will pay ransom for life or death information. (James,  Simon, 2017) One form of attack, known as a MEDJACK or medical device hijack, is  particularly effective at exploiting these weakness. Moshe Ben Simon, VP of TrapX Security,  describes it as “the attack vector of choice in healthcare…[it] is designed to rapidly penetrate  [medical] devices, establish command and control, and then use these as pivot points to hijack  and exfiltrate data from across the healthcare institution.”(TrapX, 2015, p. 5) Unfortunately,  knowing about the attack is not enough to protect hospitals from being attacked. The goal of this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be  taken in order to protect hospitals and their patients from a potentially lethal attack.   Introduction  The last thing on any patient’s mind should be the fear of their hospital being attacked by cyber  criminals. Unfortunately, over the past few years, this fear is becoming more and more of a  reality. In 2016, a hospital in Washington DC came under seige of a ransomware attack, which  paralyzed the hospital and forced them to shut down their computer system. With their record  systems offline, patients were unable to receive the care that they needed for over three days  until the system could be restored. (Cox, 2016) If that is not scary enough, the healthcare  industry saw an increase of 89% of ransomware breaches between 2016 and 2017. (TrapX, 2018)  Many of these breaches can be attributed to MEDJACK attacks, also known as medical device  hijacking. This attack vector uses malware to exploit medical device vulnerabilities to create a  backdoor into the hospitals network and gain access to valuable information. Unfortunately,  because of insecure medical devices, evolving malware, and the cyber security environment of  hospitals, MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect attack.   To the Community  The number of attacks on the healthcare industry is staggering, but it is an issue rarely talked  about. The only times people are interested is when a large attack happens, like when a hospital  gets shut down because of ransomware. The consequences of attacks can be devastating, yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it is  the “perfect storm”; it is difficult to “prevent, detect, and remediate.” (TrapX, 2015, p. 11)  Consequently, many hospital do not even know that there networks have become infected.  (TrapX, 2016) While cyber security is something that every industry is struggling with, I believe  the healthcare industry is the most in need of improvement. If an attack can cost someone their  life, it should not as easy as it is.     \n"
     ]
    }
   ],
   "source": [
    "content_of_pdf = content_of_pdf.replace('\\n', ' ')\n",
    "print(content_of_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /hkfs/work/workspace_haic/scratch/sb7059-llm_models_jeremy/Phi/Phi3/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens cache size = 323\n",
      "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.33 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    52.84 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  1140.02 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  1088.80 MiB\n",
      "............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1120\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   223.12 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   196.88 MiB\n",
      "llama_new_context_with_model: KV self size  =  420.00 MiB, K (f16):  210.00 MiB, V (f16):  210.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   138.76 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   138.77 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    14.77 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n",
      "/home/iai/sb7059/git/llm_test/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDJACK Attacks:   The Scariest Part of the Hospital\n",
      "MEDJACK Attacks:   The Scariest Part of the Hospital\n",
      "MEDJACK Attacks:   The Scariest Part of the Hospital\n",
      "MEDJACK Attacks:   The Scariest Part of the Hospital\n",
      "MEDJACK Attacks:   The Scariest Part of the Hospital\n",
      "this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be\n",
      "this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be\n",
      "this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be\n",
      "this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be\n",
      "this  paper will be to outline why MEDJACK attacks are so effective and what actions need to be\n",
      "MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect\n",
      "MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect\n",
      "MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect\n",
      "MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect\n",
      "MEDJACK attacks are difficult to prevent, detect, and remediate thus making them the  perfect\n",
      "yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it\n",
      "yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it\n",
      "yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it\n",
      "yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it\n",
      "yet they  happen often. Medical device hijacking, MEDJACK, is the main method of attack because it\n"
     ]
    }
   ],
   "source": [
    "model_path = WORKSPACE_DIC + \"/Phi/Phi3/Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "prompt_template = PromptTemplate.from_template(PROMPT)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path= model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    n_batch=512,\n",
    "    n_ctx=1100,\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_tokens = 3000,\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "#Create text splitter to split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "# Split the text into chunks\n",
    "docs = text_splitter.split_text(content_of_pdf)\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name= \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_texts(docs, embedding_function)\n",
    "\n",
    "\n",
    "\n",
    "#Retrieve and generate using the relevant snippets of the blog\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What is the most important part of a MEDJACK?\")\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "\n",
    "\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# chain = prompt_template | llm\n",
    "\n",
    "# rag_chain = (\n",
    "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# #Print the complete chain which is given to the llm\n",
    "# print(rag_chain)\n",
    "\n",
    "# rag_chain.invoke(\"When was this paper MEDJACK Attacks: The Scariest Part of the Hospital published?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
