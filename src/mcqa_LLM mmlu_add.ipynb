{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from prompt_templates import *\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = { #\"Mixtral-8x-7b\": \"../models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "               #\"Phi-2\": \"../models/phi-2.Q5_K_M.gguf\",\n",
    "               \"Llama2-70b\": \"../models/llama2_70b_chat_uncensored.Q5_K_M.gguf\",\n",
    "               #\"Yi-34b\": \"../models/yi-34b-200k.Q5_K_M.gguf\",\n",
    "               #\"Dolphin-2.5\": \"../models/dolphin-2.5-mixtral-8x7b.Q5_K_M.gguf\",\n",
    "              }\n",
    "\n",
    "#Set output file name\n",
    "OUTPUT_EVALUATION = \"../data/llm_201_301.pkl\"\n",
    "\n",
    "#Filename output evaluation detailed\n",
    "OUTPUT_EVALUATION_DETAILED = \"../data/llm_prob_result_detailed_201_301.pkl\"\n",
    "\n",
    "#Sampling rate determines how often a question is asked again if the answer format is wrong\n",
    "MAX_SAMPLING_RATE = 5\n",
    "\n",
    "#Set to 1 if you dont want to shuffle\n",
    "NUM_OF_SHUFFLES = 5\n",
    "\n",
    "#Parameter for changing the temperature of the model\n",
    "TEMPERATURE = 1\n",
    "FEW_SHOT_TEMPLATE = \"\"\"\n",
    "    The following are multiple choice questions (with answers) about network fundamentals, network access,\n",
    "    security fundamentals, automation and programmability. Here are four examples:\n",
    "    \n",
    "    Question: Which two options are the best reasons to use an IPV4 private IP space? (Choose two.)\n",
    "    \n",
    "    Choices:\n",
    "    A. to enable intra-enterprise communication\n",
    "    B. to implement NAT\n",
    "    C. to connect applications\n",
    "    D. to conserve global address space\n",
    "    E. to manage routing overhead\n",
    "                                               \n",
    "    Correct Answer: ['A','D']\n",
    "\n",
    "    Question: Security Group Access requires which three syslog messages to be sent to Cisco ISE? (Choose three.)\n",
    "    A. IOS-7-PROXY_DROP\n",
    "    B. AP-1-AUTH_PROXY_DOS_ATTACK\n",
    "    C. MKA-2-MACDROP\n",
    "    D. AUTHMGR-5-MACMOVE\n",
    "    E. ASA-6-CONNECT_BUILT\n",
    "    F. AP-1-AUTH_PROXY_FALLBACK_REQ\n",
    "\n",
    "    Correct Answer: ['B', 'D', 'F']\n",
    "\n",
    "    Question: Which two authentication stores are supported to design a wireless network using PEAP EAP-MSCHAPv2 as the authentication method? (Choose two.)\n",
    "    A. Microsoft Active Directory\n",
    "    B. ACS\n",
    "    C. LDAP\n",
    "    D. RSA Secure-ID\n",
    "    E. Certificate Server\n",
    "    \n",
    "    Correct Answer: ['A', 'B']\n",
    "\n",
    "    Question:The corporate security policy requires multiple elements to be matched in an authorization policy. Which elements can be combined to meet the requirement?\n",
    "    A. Device registration status and device activation status\n",
    "    B. Network access device and time condition\n",
    "    C. User credentials and server certificate\n",
    "    D. Built-in profile and custom profile\n",
    "\n",
    "    Correct Answer: ['B']\n",
    "\n",
    "    Question:Which three posture states can be used for authorization rules? (Choose three.)\n",
    "    A. unknown\n",
    "    B. known\n",
    "    C. noncompliant\n",
    "    D. quarantined\n",
    "    E. compliant\n",
    "    F. no access\n",
    "    G. limited\n",
    "\n",
    "    Correct Answer: ['A', 'C', 'E']\n",
    "\n",
    "    \n",
    "    Please only response with the letter(s) of the correct answer like given in the example.       \n",
    "    \n",
    "    Now, answer the following question:\n",
    "    Question: {Exam_Question}        \n",
    "                    \n",
    "    Choices:\n",
    "    {Exam_Choices}\n",
    "    \"\"\"\n",
    "\n",
    "SHUFFLE_PROB_ENB = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers (e.g., ['A', 'B']) if found, otherwise None. \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r\"\"\"\n",
    "        ^\\s*                \n",
    "        Correct\\s+Answer:\\s+  \n",
    "        \\[                  \n",
    "        ['ABCDET\\s,]+      \n",
    "        \\]                  \n",
    "        \\s*                 \n",
    "    \"\"\"\n",
    "    match = re.search(pattern, answer, flags=re.VERBOSE)\n",
    "    if match:\n",
    "        # Extract the answer portion within the brackets\n",
    "        answer_section = match.group(0).split('[')[1].split(']')[0]\n",
    "\n",
    "        # Find individual answers (consider making this more robust if needed)\n",
    "        correct_answers = answer_section.strip().split(',')\n",
    "        return [ans.strip().strip(\"'\") for ans in correct_answers]\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_answers(answerLLM, answer_exam):\n",
    "    \"\"\"Compares the extracted correct answers with the answers in answer_exam.\n",
    "\n",
    "    Keyword arguments:\n",
    "    answerLLM -- the list of answers extracted from the LLM answer\n",
    "    answer_exam -- list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the answer_exam string to a list of answers\n",
    "    answer_exam_list = answer_exam.split(\" \")\n",
    "    #Get number of correct answers in the exam\n",
    "    num_of_correct_exam_answers = len(answer_exam_list)\n",
    "    \n",
    "\n",
    "    # Convert both lists to sets for efficient comparison\n",
    "    answer_LLM_set = set(answerLLM)\n",
    "    answer_exam_set = set(answer_exam_list)\n",
    "\n",
    "    # Calculate the count of matching answers\n",
    "    number_of_correct_llm_answers = len(answer_LLM_set.intersection(answer_exam_set))\n",
    "\n",
    "    # Check if the number of answers given by the LLM is greater than the number of correct answers\n",
    "    too_many_answ_given = False\n",
    "    if len(answer_LLM_set) > num_of_correct_exam_answers:\n",
    "        too_many_answ_given = True\n",
    "\n",
    "    # Return a dictionary with the matching count and the number of correct answers\n",
    "    return number_of_correct_llm_answers, too_many_answ_given\n",
    "\n",
    "def evaluation_sampling(llm_answer, exam_Answers, num_of_correct_answer):\n",
    "    \"\"\"Analyse the answer given by the LLM and compare it with the exam answers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    llm_answer -- the answer string given by the LLM\n",
    "    exam_Answers -- the list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    answerLLM = extract_answer(llm_answer)\n",
    "    if answerLLM is not None:\n",
    "        num_of_correct_llm_Answers, too_many_answ = compare_answers(answerLLM, exam_Answers)\n",
    "        if num_of_correct_llm_Answers == num_of_correct_answer and too_many_answ == False:\n",
    "            answered_correctly = True\n",
    "        else:\n",
    "            answered_correctly = False \n",
    "        return num_of_correct_llm_Answers, answerLLM, too_many_answ, answered_correctly\n",
    "    else:\n",
    "         return -1\n",
    "\n",
    "\n",
    "#####\n",
    "# Partial Credit allowed?\n",
    "####\n",
    "def evaluation(llm_output_dataframe):\n",
    "\n",
    "    # Compute the number of total questions for each model\n",
    "    number_of_questions = llm_output_dataframe.groupby('Model')['QuestionIndex'].count()\n",
    "    \n",
    "    #Number of fully correct answers given by the LLM\n",
    "    correctly_answered = llm_output_dataframe.groupby('Model')['Answered_Correctly'].sum()\n",
    "\n",
    "    #Number of incorrect answers given by the LLM\n",
    "    incorrectly_answered = number_of_questions - correctly_answered\n",
    "\n",
    "    #Amount of correct answers in the exam\n",
    "    amount_correcct_exam_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectExamAnswers'].sum()\n",
    "\n",
    "    #Amount of correct answers given by the LLM even if not fully correct\n",
    "    amount_correcct_llm_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectLLMAnswers'].sum()\n",
    "    \n",
    "    #Calculation of Accuracy and Recall and f1 score\n",
    "    accuracy = correctly_answered / number_of_questions\n",
    "    accuracy_partial = amount_correcct_llm_answers / amount_correcct_exam_answers\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Number of Questions': number_of_questions,\n",
    "        'Correctly Answered': correctly_answered,\n",
    "        'Incorrectly Answered': incorrectly_answered,\n",
    "        'Accuracy': accuracy,\n",
    "        'Accuracy Partial': accuracy_partial,\n",
    "    })\n",
    "\n",
    "    results_df = results_df.reset_index()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_evaluation(evaluation_df):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from a DataFrame containing columns:\n",
    "        - 'Model'\n",
    "        - 'Accuracy Mean', 'Accuracy Min', 'Accuracy Max'\n",
    "        - 'Accuracy Partial Mean', 'Accuracy Partial Min', 'Accuracy Partial Max'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a list of colors for the models\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    # Define bar width\n",
    "    bar_width = 0.5  # Increase bar width for thicker bars\n",
    "\n",
    "    # --- Subplot 1: Accuracy ---\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[0].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Mean'], bar_width, \n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Mean'] - evaluation_df.loc[i, 'Accuracy Min'])], [abs(evaluation_df.loc[i, 'Accuracy Max'] - evaluation_df.loc[i, 'Accuracy Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[0].set_ylabel('Accuracy (%)')\n",
    "    axs[0].set_title('Accuracy Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[0].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[0].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylim([0, 1])\n",
    "    axs[0].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[0].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # --- Subplot 2: Partial Accuracy ---\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[1].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Partial Mean'], bar_width,\n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Partial Mean'] - evaluation_df.loc[i, 'Accuracy Partial Min'])], [abs(evaluation_df.loc[i, 'Accuracy Partial Max'] - evaluation_df.loc[i, 'Accuracy Partial Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[1].set_ylabel('Accuracy Partial (%)')\n",
    "    axs[1].set_title('Accuracy Partial Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[1].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[1].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylim([0, 1])\n",
    "    axs[1].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[1].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    fig.tight_layout(pad=1.2)  # Decrease padding for closer plots\n",
    "    plt.show()\n",
    "\n",
    "def extract_answer_from_text_file(path_to_text_file):\n",
    "   # Read the text file\n",
    "    with open(path_to_text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    questions = []\n",
    "    answers = []\n",
    "    correct_answers = []\n",
    "\n",
    "    # Parse the lines\n",
    "    question = \"\"\n",
    "    options = \"\"\n",
    "    correct_option = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Question:\"):\n",
    "            if question:  # If it's not the first question\n",
    "                questions.append(question)\n",
    "                answers.append(options)\n",
    "                correct_answers.append(correct_option)\n",
    "                options = \"\"  # Reset options for the new question\n",
    "            question = line.replace(\"Question:\", \"\").strip()\n",
    "        elif line.startswith(\"Answer:\"):\n",
    "            correct_option = line.split(\":\")[-1].strip()\n",
    "        elif line.startswith((\"A.\", \"B.\", \"C.\", \"D.\", \"E.\", \"F.\", \"G.\")):\n",
    "            options += line.strip() + \" \"  # Add options to the string\n",
    "\n",
    "    # Append the last question\n",
    "    questions.append(question)\n",
    "    answers.append(options)\n",
    "    correct_answers.append(correct_option)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Question': questions,\n",
    "        'Answers': answers,\n",
    "        'Correct Answer': correct_answers\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_options_string(options_string):\n",
    "    # Use regular expression to split the string\n",
    "    options_list = re.split(r'\\n(?=[A-Z]\\. )', options_string)\n",
    "    \n",
    "    # Remove any leading or trailing whitespace from each option\n",
    "    options_list = [option.strip() for option in options_list if option.strip()]\n",
    "    \n",
    "    return options_list\n",
    "\n",
    "def format_options_string(options_list):\n",
    "    # Join the options list with newline characters\n",
    "    formatted_options_string = '\\n'.join(options_list)\n",
    "    \n",
    "    return formatted_options_string\n",
    "\n",
    "\n",
    "def evaluation_probability(char_prob_list, llm_answer, exam_answers, num_of_correct_answer):\n",
    "    # Normalize probabilities\n",
    "    total = sum(char_prob_list.values())\n",
    "    for letter in char_prob_list:\n",
    "        char_prob_list[letter] /= total\n",
    "\n",
    "\n",
    "    # Sort characters by probability\n",
    "    sorted_chars = sorted(char_prob_list.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select answers based on the number of correct answers for the question\n",
    "    answerLLm = [char for char, prob in sorted_chars[:num_of_correct_answer]]\n",
    "    num_of_correct_llm_answer, too_many_answers = compare_answers(answerLLm, exam_answers)\n",
    "    \n",
    "    if num_of_correct_llm_answer == num_of_correct_answer and too_many_answers == False:\n",
    "        answered_correctly = True\n",
    "    else :\n",
    "        answered_correctly = False\n",
    "    return num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly\n",
    "\n",
    "def extract_exam_info(examQuestion):\n",
    "    #Extracting the question, answer and choices from the dataframe\n",
    "    question = examQuestion[0].strip()  # Get the value of the first column (question) and remove leading/trailing whitespace\n",
    "    answer_exam = examQuestion[2].strip()    # Get the value of the third column (answer) and remove leading/trailing whitespace\n",
    "    answer_exam_with_whitespace = ' '.join(list(answer_exam))\n",
    "\n",
    "    #Set the number of correct answers\n",
    "    num_of_correct_answer = len([char for char in answer_exam_with_whitespace if char != ' '])\n",
    "\n",
    "    #Create the choices string like in the prompt template\n",
    "    choices = \"\"  # Reset the choices variable for each question\n",
    "    choices = split_options_string(examQuestion[1])\n",
    "    choices = format_options_string(choices)\n",
    "\n",
    "    return question, choices, answer_exam_with_whitespace, num_of_correct_answer\n",
    "\n",
    "\n",
    "def shuffle_choices(choices_string, answers):\n",
    "    # Split the input string into a list of choices\n",
    "    choices = re.split(r'\\n(?=[A-Z]\\. )', choices_string)\n",
    "    \n",
    "    # Create a dictionary to track the original and new letters of the choices\n",
    "    letter_map = {}\n",
    "    \n",
    "    # Shuffle the list\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    # Reassign the choice letters\n",
    "    shuffled_string = \"\"\n",
    "    for i, choice in enumerate(choices):\n",
    "        original_letter, rest_of_choice = choice.split(\". \", 1)\n",
    "        new_letter = chr(65 + i)\n",
    "        shuffled_string += new_letter + \". \" + rest_of_choice + \"\\n\"\n",
    "        \n",
    "        # Update the letter map\n",
    "        letter_map[original_letter] = new_letter\n",
    "    \n",
    "    # Split the answers string into a list of answer letters\n",
    "    answers = answers.split()\n",
    "    \n",
    "    # Update the answers\n",
    "    new_answers = [letter_map[answer] for answer in answers]\n",
    "    \n",
    "    return shuffled_string.strip(), ' '.join(new_answers)\n",
    "\n",
    "def calculate_model_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each model in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame containing evaluation metrics for different models.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: New DataFrame containing calculated statistics for each model.\n",
    "    \"\"\"\n",
    "    model_stats = []\n",
    "    for model, group_df in df.groupby('Model'):\n",
    "        model_stat = {\n",
    "            'Model': model,\n",
    "            'Accuracy Mean': group_df['Accuracy'].mean(),\n",
    "            'Accuracy Max': group_df['Accuracy'].max(),\n",
    "            'Accuracy Min': group_df['Accuracy'].min(),\n",
    "            'Accuracy STD': group_df['Accuracy'].std(),\n",
    "            'Accuracy Partial Mean': group_df['Accuracy Partial'].mean(),\n",
    "            'Accuracy Partial Max': group_df['Accuracy Partial'].max(),\n",
    "            'Accuracy Partial Min': group_df['Accuracy Partial'].min(),\n",
    "            'Accuracy Partial STD': group_df['Accuracy Partial'].std()\n",
    "        }\n",
    "        model_stats.append(model_stat)\n",
    "    \n",
    "    return pd.DataFrame(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_question_answer = False  \n",
    "#Create a dataframe with the size of NUM_OF_SHUFFLES which contains the dataframe llm_exam_result\n",
    "shuffled_evalutation_df = pd.DataFrame(columns=[ 'Number of Questions','Correctly Answered','Incorrectly Answered','Accuracy','Accuracy Partial'])\n",
    "questions  = pd.read_excel(\"../data/201-301-CCNA.xlsx\")\n",
    "#Take the first 20 questions\n",
    "questions = questions.head(20)\n",
    "\n",
    "#questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "prompt_template = PromptTemplate.from_template(FEW_SHOT_TEMPLATE)\n",
    "\n",
    "#Iterate over each model definied in the MODEL_PATH dictionary\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "     #Load the model wiht LLamaCpp\n",
    "    llm = LlamaCpp(\n",
    "        model_path= model_path,\n",
    "        n_gpu_layers=128,\n",
    "        n_batch=512,\n",
    "        n_ctx=1100,\n",
    "        temperature=1,\n",
    "        top_p=0.9,\n",
    "        #max_tokens = 100,\n",
    "        #callback_manager=callback_manager,\n",
    "        verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.DataFrame(columns = [\"Model\", \"QuestionIndex\", \"SamplingIndex\", \"NumberOfCorrectLLMAnswers\", \"NumberOfCorrectExamAnswers\", \"Ratio\", \"LLM_Answer\", \"Exam_Answers\", \"Answered_Correctly\",  \"Too_Many_answers\"]) \n",
    "        #Iterate over each question in the question dataframe\n",
    "        for index_question, row in questions.iterrows():\n",
    "            #Extract the question, answer and choices from the dataframe\n",
    "            question, choices, answers, num_of_correct_answer = extract_exam_info(row)\n",
    "            #Only if shuffle is enabled, shuffle the choices\n",
    "            if shuffled_iteration > 0:\n",
    "                choices, answers = shuffle_choices(choices, answers)\n",
    "            #Empty the char_probabilities dictionary for each question\n",
    "            char_probabilities = {}\n",
    "\n",
    "            #Iterate over the maximum sampling rate\n",
    "            for index_sampling in range(MAX_SAMPLING_RATE):\n",
    "                # Invoke the chain with the question and choices\n",
    "                llm_answer = chain.invoke({\"Exam_Question\" : question, \"Exam_Choices\" : choices})            \n",
    "\n",
    "                # Check if the answer is in the expected format\n",
    "                if extract_answer(llm_answer) is not None:\n",
    "\n",
    "                    #If probability is enabled, check what the correct answer is after all the sampling\n",
    "                    if not SHUFFLE_PROB_ENB:\n",
    "                        # Extract the correct answers from the LLM answer and analyse the answer\n",
    "                        num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly = evaluation_sampling(llm_answer, answers, num_of_correct_answer)\n",
    "                        #Save the current sampling index -- How of the question has been asked until the answer was in the correct format\n",
    "                        sample_Index = index_sampling\n",
    "                        valid_question_answer = True\n",
    "                        break\n",
    "                    else:\n",
    "                        answer_letters = extract_answer(llm_answer)\n",
    "                        sample_Index = index_sampling\n",
    "                        valid_question_answer = True\n",
    "                        #Shuffle the choices\n",
    "                        choices = shuffle_choices(choices, answers)\n",
    "                        for letter in answer_letters:\n",
    "                            if letter in char_probabilities:\n",
    "                                char_probabilities[letter] += 1\n",
    "                            else:\n",
    "                                char_probabilities[letter] = 1\n",
    "            \n",
    "            if SHUFFLE_PROB_ENB:\n",
    "                num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly = evaluation_probability(char_probabilities, llm_answer, answers, num_of_correct_answer)\n",
    "        \n",
    "                        \n",
    "            #Depending on the result of the answer, add the result to the dataframe\n",
    "            if not valid_question_answer:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [-1], \"NumberOfCorrectLLMAnswers\": [0], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [-1], \"LLM_Answer\": [llm_answer], \"Exam_Answers\": [answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "            else:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [sample_Index], \"NumberOfCorrectLLMAnswers\": [num_of_correct_llm_answer], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [num_of_correct_llm_answer/num_of_correct_answer], \"LLM_Answer\": [answerLLm], \"Exam_Answers\": [answers], \"Answered_Correctly\" : [answered_correctly], \"Too_Many_answers\": [too_many_answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "                valid_question_answer = False\n",
    "        answered_correctly = False\n",
    "        #Concat the the dataframe returned by evaulation to one dataframe\n",
    "        display(llm_exam_result)\n",
    "        #llm_exam_result.to_pickle(f\"../data/{model}_shuffled_{shuffled_iteration}_201_301.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "        display(shuffled_evalutation_df)\n",
    "\n",
    "#plot_evaluation(shuffled_evalutation_df)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "plot_evaluation(model_statistics)\n",
    "shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "#model_statistics.to_pickle(OUTPUT_EVALUATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_path in MODEL_PATH.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.read_pickle(f\"../data/{model}_shuffled_{shuffled_iteration}_201_301.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "plot_evaluation(model_statistics)\n",
    "shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "model_statistics.to_pickle(OUTPUT_EVALUATION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
