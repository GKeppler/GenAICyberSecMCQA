{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "### Dataset MMLU Computer Security:\n",
    "* https://huggingface.co/datasets/cais/mmlu/viewer/computer_security/test\n",
    "\n",
    "### Official HELM Result MMLU Computer Security\n",
    "https://crfm.stanford.edu/helm/mmlu/latest/#/leaderboard\n",
    "\n",
    "### Big Mistake in Prompt:\n",
    "**Added the \"\"\" after a linebreak e.g.** <br>\n",
    "Anwser: <br>\n",
    "\"\"\"<br>\n",
    "**Correct way:<br>**\n",
    "Anwser:\"\"\"\n",
    "\n",
    "## Cisco Pass exam\n",
    "* https://learningnetwork.cisco.com/s/question/0D53i00000U2TO7CAN/what-is-the-pass-percentage-required-to-get-in-ccna-200301\n",
    "* 825 / 1000 (82 %)\n",
    "* Every (paid) exam takes 120 min and has about 120 Questions\n",
    "\n",
    "## HELM  implementation: \n",
    "* Max_output tokens to restrict the possible outputs to the max number of answers possibilities\n",
    "* Temperature = 0\n",
    "* Joint strategy (all answer choices are presented at once)\n",
    "* Short Introduction: The following are multiple choice questions (with answers) about computer security.\n",
    "* (Paper: https://arxiv.org/pdf/2211.09110.pdf)\n",
    "\n",
    "## HELM / Paper Imprreovements:\n",
    "* Sampling --> std, mean min / max accuracy\n",
    "\n",
    "# CCNA 201-301 - 5 Shot like HELM with Answer format: Answer: ABC or Answer: A <br>\n",
    "### Oberservations Phi:\n",
    "* Output of Phi-Model always empty, similiar config as mmlu (temp=0, max_output_token = 2), by increasing temp the result of Phi is better but still not close to the other models<br>\n",
    "* Also have to increase the Max Output Tokens otherwise only \\n as response\n",
    "* Possible approach: Increasing temp and output_tokens, regex pattern that searchs for string in responses\n",
    "### Oberservations Llama 2:\n",
    "* LLama 2 results are much worse in comp. to single shot with Correct Answer: ['A', 'B'] and temp =0.7 and no limit to output tokens\n",
    "\n",
    "\n",
    "## New \n",
    "\n",
    "* ~~Yi, Llama 1, Llama 2 (nicht uncensored), Mixtral, Phi-2 --> Für MMLU~~\n",
    "* Change Answer to Letter (Frage: Exam Answer sind in HELM so, doch ändern?)\n",
    "* ~~Prompt without whitespace --> extra ~~\n",
    "* Ungeshuffeld vs HELM Grafik --> HELM vs unsere\n",
    "* Shuffle (min-max) vs nicht shuffeln\n",
    "* Dokumentieren --> Prompt, Changes Correct Answer Helm implementierung\n",
    "* Folien für Präsentation --> Implementierung, Ergebnisse (MMLU ähnlich wie HELM) --> , mit mehr Tokens CCNA Paper (Questionsbank nicht veröffentlicht) --> Weg Cisco (350-701 SCOR), Vetgleich zum Paper\n",
    "* Alles Dokumentieren\n",
    "* **How many points to pass? --> Horizontale** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import *\n",
    "import time\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import yaml\n",
    "import json\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set the output limit to inf\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = 'config_ccna.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#Functions for the evaluation of the LLM\n",
    "########################################\n",
    "\n",
    "def load_config(config_paht):\n",
    "    \"\"\"Loads the LLM model configuration from the provided path.\n",
    "\n",
    "    Args:\n",
    "        config_path: The path to the LLM model configuration file.\n",
    "\n",
    "    Returns:\n",
    "        The LLM model configuration.\n",
    "    \"\"\"\n",
    "    with open(config_paht, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "def extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers (e.g., ['A', 'B']) if found, otherwise None. \n",
    "    \"\"\"\n",
    "    #print(repr(answer))\n",
    "    answer = re.sub(r'[\\s\\n.,]', '', answer)\n",
    "    pattern = re.compile(r'^[A-Z,]*$')\n",
    "    #print(answer)\n",
    "    if re.match(pattern, answer):\n",
    "        if ',' in answer:\n",
    "            return None\n",
    "        else:\n",
    "            return list(answer)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def compare_answers(answerLLM, answer_exam):\n",
    "    \"\"\"Compares the extracted correct answers with the answers in answer_exam.\n",
    "\n",
    "    Keyword arguments:\n",
    "    answerLLM -- the list of answers extracted from the LLM answer\n",
    "    answer_exam -- list of answers from the exam\n",
    "    \"\"\"\n",
    "    # Convert answer_exam_list from letters to numbers\n",
    "    answerLLM = [ord(answer) - 65 for answer in answerLLM]\n",
    "\n",
    "    # Get number of correct answers in the exam\n",
    "    num_of_correct_exam_answers = len(answer_exam)\n",
    "\n",
    "    # Convert both lists to sets for efficient comparison\n",
    "    answer_LLM_set = set(answerLLM)\n",
    "    answer_exam_set = set(answer_exam)\n",
    "\n",
    "    # Calculate the count of matching answers\n",
    "    number_of_correct_llm_answers = len(answer_LLM_set.intersection(answer_exam_set))\n",
    "\n",
    "    #Calculate the number of incorrect answers\n",
    "    number_of_incorrect_llm_answers = len(answer_LLM_set.difference(answer_exam_set))\n",
    "\n",
    "    # Check if the number of answers given by the LLM is greater than the number of correct answers\n",
    "    too_many_answ_given = False\n",
    "    if len(answer_LLM_set) > num_of_correct_exam_answers:\n",
    "        too_many_answ_given = True\n",
    "\n",
    "    # Return a dictionary with the matching count and the number of correct answers\n",
    "    return number_of_correct_llm_answers, too_many_answ_given, number_of_incorrect_llm_answers\n",
    "\n",
    "def format_choices_for_llm(choices):\n",
    "    #Define the letters for the choices\n",
    "    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    \n",
    "    # Erstellen Sie den formatierten String\n",
    "    formatted_choices = '\\n'.join(f'{letters[i]}. {choice}' for i, choice in enumerate(choices))\n",
    "    \n",
    "    return formatted_choices\n",
    "\n",
    "def evaluation_sampling(llm_answer, exam_Answers, num_of_correct_answer):\n",
    "    \"\"\"Analyse the answer given by the LLM and compare it with the exam answers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    llm_answer -- the answer string given by the LLM\n",
    "    exam_Answers -- the list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    answerLLM = extract_answer(llm_answer)\n",
    "    if answerLLM is not None:\n",
    "        num_of_correct_llm_Answers, too_many_answ, number_of_incorrect_answers = compare_answers(answerLLM, exam_Answers)\n",
    "        if num_of_correct_llm_Answers == num_of_correct_answer and too_many_answ == False:\n",
    "            answered_correctly = True\n",
    "        else:\n",
    "            answered_correctly = False \n",
    "        return num_of_correct_llm_Answers, answerLLM, too_many_answ, answered_correctly, number_of_incorrect_answers\n",
    "    else:\n",
    "         return -1\n",
    "\n",
    "\n",
    "def evaluation(llm_output_dataframe):\n",
    "\n",
    "    # Compute the number of total questions for each model\n",
    "    number_of_questions = llm_output_dataframe.groupby('Model')['QuestionIndex'].count()\n",
    "    \n",
    "    #Number of fully correct answers given by the LLM\n",
    "    correctly_answered = llm_output_dataframe.groupby('Model')['Answered_Correctly'].sum()\n",
    "\n",
    "    #Number of incorrect answers given by the LLM\n",
    "    incorrectly_answered = number_of_questions - correctly_answered\n",
    "\n",
    "    #Amount of correct answers in the exam\n",
    "    amount_correct_exam_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectExamAnswers'].sum()\n",
    "\n",
    "    #Amount of correct answers given by the LLM even if not fully correct\n",
    "    amount_correct_llm_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectLLMAnswers'].sum()\n",
    "    \n",
    "    # Calculate Partial Credits\n",
    "    llm_output_dataframe['Partial_Credit'] = llm_output_dataframe.apply(\n",
    "        lambda row: max(0, row['NumberOfCorrectLLMAnswers'] / row['NumberOfCorrectExamAnswers'] - \n",
    "                        (row['NumberOfIncorrectAnswers'] /(row['NumberOfChoices']-row['NumberOfCorrectExamAnswers']))), axis=1)\n",
    "    \n",
    "    # Aggregate Partial Credit for each model\n",
    "    partial_credit_sum = llm_output_dataframe.groupby('Model')['Partial_Credit'].sum()\n",
    "\n",
    "    #Calculation of Accuracy and Recall and f1 score\n",
    "    accuracy = correctly_answered / number_of_questions\n",
    "    accuracy_partial = partial_credit_sum / number_of_questions    \n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Number of Questions': number_of_questions,\n",
    "        'Correctly Answered': correctly_answered,\n",
    "        'Incorrectly Answered': incorrectly_answered,\n",
    "        'Accuracy': accuracy,\n",
    "        'Accuracy Partial': accuracy_partial,\n",
    "        'Total Partial Credit': partial_credit_sum\n",
    "    })\n",
    "\n",
    "    results_df = results_df.reset_index()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_evaluation_CCNA(evaluation_df, hline_accuracy=None, hline_partial=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from a DataFrame containing columns:\n",
    "        - 'Model'\n",
    "        - 'Accuracy Mean', 'Accuracy Min', 'Accuracy Max'\n",
    "        - 'Accuracy Partial Mean', 'Accuracy Partial Min', 'Accuracy Partial Max'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a list of colors for the models\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    # Define bar width\n",
    "    bar_width = 0.5  # Increase bar width for thicker bars\n",
    "\n",
    "    # --- Subplot 1: Accuracy ---\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[0].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Mean'], bar_width, \n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Mean'] - evaluation_df.loc[i, 'Accuracy Min'])], [abs(evaluation_df.loc[i, 'Accuracy Max'] - evaluation_df.loc[i, 'Accuracy Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[0].set_ylabel('Accuracy (%)')\n",
    "    axs[0].set_title('Accuracy Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[0].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[0].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylim([0, 1])\n",
    "    axs[0].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[0].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Accuracy subplot\n",
    "    if hline_accuracy is not None:\n",
    "        axs[0].axhline(y=hline_accuracy, color='r', linestyle='--')\n",
    "\n",
    "    # --- Subplot 2: Partial Accuracy ---\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[1].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Partial Mean'], bar_width,\n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Partial Mean'] - evaluation_df.loc[i, 'Accuracy Partial Min'])], [abs(evaluation_df.loc[i, 'Accuracy Partial Max'] - evaluation_df.loc[i, 'Accuracy Partial Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[1].set_ylabel('Accuracy Partial (%)')\n",
    "    axs[1].set_title('Accuracy Partial Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[1].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[1].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylim([0, 1])\n",
    "    axs[1].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[1].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Partial Accuracy subplot\n",
    "    if hline_partial is not None:\n",
    "        axs[1].axhline(y=hline_partial, color='r', linestyle='--')\n",
    "\n",
    "    fig.tight_layout(pad=1.2)  # Decrease padding for closer plots\n",
    "\n",
    "    # Add title to the figure\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16, y=1.05)\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(f\"{OUTPUT_PATH}llm_5_Shot_{DATASET_NAME}.png\")\n",
    "\n",
    "def calculate_model_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each model in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame containing evaluation metrics for different models.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: New DataFrame containing calculated statistics for each model.\n",
    "    \"\"\"\n",
    "    model_stats = []\n",
    "    for model, group_df in df.groupby('Model'):\n",
    "        model_stat = {\n",
    "            'Model': model,\n",
    "            'Accuracy Mean': group_df['Accuracy'].mean(),\n",
    "            'Accuracy Max': group_df['Accuracy'].max(),\n",
    "            'Accuracy Min': group_df['Accuracy'].min(),\n",
    "            'Accuracy STD': group_df['Accuracy'].std(),\n",
    "            'Accuracy Partial Mean': group_df['Accuracy Partial'].mean(),\n",
    "            'Accuracy Partial Max': group_df['Accuracy Partial'].max(),\n",
    "            'Accuracy Partial Min': group_df['Accuracy Partial'].min(),\n",
    "            'Accuracy Partial STD': group_df['Accuracy Partial'].std()\n",
    "        }\n",
    "        model_stats.append(model_stat)\n",
    "    \n",
    "    return pd.DataFrame(model_stats)\n",
    "\n",
    "\n",
    "def shuffle_choices_and_update_answer(choices, answer):\n",
    "    # Erstellen Sie eine Liste von Indizes und mischen Sie sie\n",
    "    indices = list(range(len(choices)))\n",
    "    random.shuffle(indices)\n",
    "    shuffled_choices = [choices[i] for i in indices]\n",
    "    updated_answer = [indices.index(a) for a in answer]  \n",
    "    \n",
    "    return shuffled_choices, updated_answer\n",
    "\n",
    "def plot_evaluation_MMLU(llm_result_df, helm_result, df1_name, df2_name, title=None, probability=0.25):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from two DataFrames containing columns:\n",
    "        - 'Model'\n",
    "        - 'Accuracy'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define colors for the models\n",
    "    color_llm_result = 'b'\n",
    "    color_helm_result = 'orange'\n",
    "\n",
    "    # Define bar width and gap\n",
    "    bar_width = 0.2  # Decrease bar width for side-by-side bars with a gap\n",
    "    gap = 0.05  # Define gap between bars\n",
    "\n",
    "    #Merge the two DataFrames, so that the results can be compared (Official HELM Results vs LLM Results)\n",
    "    llm_result_df = pd.merge(llm_result_df, helm_result, on='Model', suffixes=('_LLM', '_HELM'))\n",
    "    llm_result_df = llm_result_df.rename(columns={'Accuracy': 'Accuracy_HELM', 'Accuracy Mean': 'Accuracy_Mean_LLM'})\n",
    "\n",
    "    display(llm_result_df)\n",
    "\n",
    "    # Just one plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    for i, model in enumerate(llm_result_df['Model']):\n",
    "        bars_llm = ax.bar(i - bar_width - gap / 2, llm_result_df.loc[i, 'Accuracy_Mean_LLM'], bar_width, color=color_llm_result)\n",
    "        bars_helm = ax.bar(i + gap / 2, llm_result_df.loc[i, 'Accuracy_HELM'], bar_width, color=color_helm_result)\n",
    "\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Accuracy Mean LLM vs. HELM', fontsize=12)\n",
    "    ax.set_xticks([i for i in range(len(llm_result_df['Model']))])\n",
    "    ax.set_xticklabels(llm_result_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    ax.legend([bars_llm, bars_helm], [df1_name, df2_name])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Accuracy subplot\n",
    "    ax.axhline(y=probability, color='r', linestyle='--')\n",
    "\n",
    "    # Add title to the figure\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16, y=1.05)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_50/llm_prob_result_201-301-CCNA_5_Shot.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 65\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRACK_RESULTS:\n\u001b[1;32m     55\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUN_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m: DATASET_NAME, \n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m: time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAX_TOKENS\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAX_OUTPUT_TOKENS  \n\u001b[1;32m     64\u001b[0m     }\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_EVALUATION_JSON\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     66\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(parameters, f)\n",
      "File \u001b[0;32m~/git/llm_test/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_50/llm_prob_result_201-301-CCNA_5_Shot.json'"
     ]
    }
   ],
   "source": [
    "config = load_config(CONFIG_FILE)\n",
    "\n",
    "\n",
    "# Assign values from the configuration\n",
    "WORKSPACE_DIC = config['workspace_dir']\n",
    "MODEL_PATH = config['model_paths']\n",
    "TEMPERATURE = config['model_parameters']['temperature']\n",
    "MAX_OUTPUT_TOKENS = config['model_parameters']['max_output_tokens']\n",
    "MAX_SAMPLING_RATE = config['max_sampling_rate']\n",
    "NUM_OF_SHUFFLES = config['num_of_shuffles']\n",
    "NUMBER_OF_QUESTIONS = config['number_of_questions']\n",
    "TRACK_RESULTS = config['track_results']\n",
    "PRINT_RESULTS = config['print_results']\n",
    "DATASET_NAME = config['dataset_name']\n",
    "DATE = time.strftime(config['date_format'])\n",
    "CONTROL_OUTPUT = config['control_output']\n",
    "\n",
    "OUTPUT_PATH = config['output_path'].format(\n",
    "    number_of_questions=NUMBER_OF_QUESTIONS,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    date=DATE\n",
    ")\n",
    "\n",
    "OUTPUT_EVALUATION = config['output_evaluation'].format(\n",
    "    output_path=OUTPUT_PATH,\n",
    "    dataset_name=DATASET_NAME\n",
    ")\n",
    "\n",
    "OUTPUT_EVALUATION_DETAILED = config['output_evaluation_detailed'].format(\n",
    "    number_of_questions=NUMBER_OF_QUESTIONS,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    date=DATE\n",
    ")\n",
    "\n",
    "OUTPUT_EVALUATION_JSON = config['output_evaluation_json'].format(\n",
    "    number_of_questions=NUMBER_OF_QUESTIONS,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    date=DATE\n",
    ")\n",
    "\n",
    "if config['create_results_folder'] and not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "QUESTIONS_BANK = config['questions_bank']\n",
    "HELM_RESULT = pd.read_pickle(config['helm_result'])\n",
    "PROMPT_TEMPLATE = config['prompt_template']\n",
    "\n",
    "#Gets the variable from the templates.py file\n",
    "PROMPT_TEMPLATE = globals()[PROMPT_TEMPLATE]\n",
    "\n",
    "#Save the metadata as a JSON file, if TRACK_RESULTS is set to True\n",
    "if TRACK_RESULTS:\n",
    "    parameters = {\n",
    "        \"RUN_NAME\": DATASET_NAME, \n",
    "        \"DATE\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),  \n",
    "        \"QUESTION_BANK\": QUESTIONS_BANK,  \n",
    "        \"MAX_SAMPLING_RATE\": MAX_SAMPLING_RATE,  \n",
    "        \"NUM_OF_SHUFFLES\": NUM_OF_SHUFFLES,  \n",
    "        \"FEW_SHOT_TEMPLATE\": PROMPT_TEMPLATE,  \n",
    "        \"TEMPERATURE\": TEMPERATURE,  \n",
    "        \"MAX_TOKENS\": MAX_OUTPUT_TOKENS  \n",
    "    }\n",
    "    with open(OUTPUT_EVALUATION_JSON, 'w') as f:\n",
    "        json.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "RUN_NAME: 201-301-CCNA\n",
      "DATE: 2024-07-21 19:50:00\n",
      "QUESTION_BANK: ../data/201-301-CCNA.parquet\n",
      "MAX_SAMPLING_RATE: 5\n",
      "NUM_OF_SHUFFLES: 1\n",
      "FEW_SHOT_TEMPLATE: The following are multiple choice questions (with answers) about network fundamentals, network access,\n",
      "security fundamentals, automation and programmability.\n",
      "\n",
      "Question: Which two options are the best reasons to use an IPV4 private IP space? (Choose two.)\n",
      "A. to enable intra-enterprise communication\n",
      "B. to implement NAT\n",
      "C. to connect applications\n",
      "D. to conserve global address space\n",
      "E. to manage routing overhead\n",
      "Answer: AD\n",
      "\n",
      "Question: Security Group Access requires which three syslog messages to be sent to Cisco ISE? (Choose three.)\n",
      "A. IOS-7-PROXY_DROP\n",
      "B. AP-1-AUTH_PROXY_DOS_ATTACK\n",
      "C. MKA-2-MACDROP\n",
      "D. AUTHMGR-5-MACMOVE\n",
      "E. ASA-6-CONNECT_BUILT\n",
      "F. AP-1-AUTH_PROXY_FALLBACK_REQ\n",
      "Answer: BDF\n",
      "\n",
      "Question: Which two authentication stores are supported to design a wireless network using PEAP EAP-MSCHAPv2 as the authentication method? (Choose two.)\n",
      "A. Microsoft Active Directory\n",
      "B. ACS\n",
      "C. LDAP\n",
      "D. RSA Secure-ID\n",
      "E. Certificate Server\n",
      "Answer: AB\n",
      "\n",
      "Question: The corporate security policy requires multiple elements to be matched in an authorization policy. Which elements can be combined to meet the requirement?\n",
      "A. Device registration status and device activation status\n",
      "B. Network access device and time condition\n",
      "C. User credentials and server certificate\n",
      "D. Built-in profile and custom profile\n",
      "Answer: B\n",
      "\n",
      "Question: Which three posture states can be used for authorization rules? (Choose three.)\n",
      "A. unknown\n",
      "B. known\n",
      "C. noncompliant\n",
      "D. quarantined\n",
      "E. compliant\n",
      "F. no access\n",
      "G. limited\n",
      "Answer: ACE\n",
      "\n",
      "Question: {Exam_Question}\n",
      "{Exam_Choices}\n",
      "Answer:\n",
      "TEMPERATURE: 0\n",
      "MAX_TOKENS: 2\n",
      "OUTPUT_PATH: ../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/\n",
      "OUTPUT_EVALUATION: ../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/llm_5_Shot_201-301-CCNA.pkl\n",
      "OUTPUT_EVALUATION_DETAILED: ../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/llm_prob_result_detailed_201-301-CCNA_5_Shot.pkl\n",
      "OUTPUT_EVALUATION_JSON: ../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/llm_prob_result_201-301-CCNA_5_Shot.json\n",
      "Model: Phi-3-medium-128k, Path: /hkfs/work/workspace_haic/scratch/sb7059-llm_models_jeremy/Phi/Phi3/Phi-3-mini-4k-instruct-q4.gguf\n"
     ]
    }
   ],
   "source": [
    "#Print all the parameters if CONTROL_OUTPUT is set to True\n",
    "if CONTROL_OUTPUT:\n",
    "    print(\"Parameters:\")\n",
    "    print(\"RUN_NAME:\", DATASET_NAME)\n",
    "    print(\"DATE:\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"QUESTION_BANK:\", QUESTIONS_BANK)\n",
    "    print(\"MAX_SAMPLING_RATE:\", MAX_SAMPLING_RATE)\n",
    "    print(\"NUM_OF_SHUFFLES:\", NUM_OF_SHUFFLES)\n",
    "    print(\"FEW_SHOT_TEMPLATE:\", PROMPT_TEMPLATE)\n",
    "    print(\"TEMPERATURE:\", TEMPERATURE)\n",
    "    print(\"MAX_TOKENS:\", MAX_OUTPUT_TOKENS)\n",
    "    print(\"OUTPUT_PATH:\", OUTPUT_PATH)\n",
    "    print(\"OUTPUT_EVALUATION:\", OUTPUT_EVALUATION)\n",
    "    print(\"OUTPUT_EVALUATION_DETAILED:\", OUTPUT_EVALUATION_DETAILED)\n",
    "    print(\"OUTPUT_EVALUATION_JSON:\", OUTPUT_EVALUATION_JSON)\n",
    "    \n",
    "    for model, path in MODEL_PATH.items():\n",
    "        print(f\"Model: {model}, Path: {path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Model ...\n",
      "Model loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>QuestionIndex</th>\n",
       "      <th>SamplingIndex</th>\n",
       "      <th>NumberOfChoices</th>\n",
       "      <th>NumberOfCorrectLLMAnswers</th>\n",
       "      <th>NumberOfIncorrectAnswers</th>\n",
       "      <th>NumberOfCorrectExamAnswers</th>\n",
       "      <th>Ratio</th>\n",
       "      <th>LLM_Answer</th>\n",
       "      <th>Exam_Answers</th>\n",
       "      <th>Answered_Correctly</th>\n",
       "      <th>Too_Many_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[B]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[B]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[C]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[B]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[C, D, E]</td>\n",
       "      <td>[2, 4]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[D]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[B]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[B]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[D]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model QuestionIndex SamplingIndex NumberOfChoices  \\\n",
       "0    Phi-3-medium-128k           181             0               4   \n",
       "1    Phi-3-medium-128k           198             0               4   \n",
       "2    Phi-3-medium-128k           121             0               4   \n",
       "3    Phi-3-medium-128k            94             0               4   \n",
       "4    Phi-3-medium-128k            46             0               5   \n",
       "..                 ...           ...           ...             ...   \n",
       "115  Phi-3-medium-128k            57             0               4   \n",
       "116  Phi-3-medium-128k            58             0               4   \n",
       "117  Phi-3-medium-128k           165             0               4   \n",
       "118  Phi-3-medium-128k            67             0               4   \n",
       "119  Phi-3-medium-128k           135             0               4   \n",
       "\n",
       "    NumberOfCorrectLLMAnswers NumberOfIncorrectAnswers  \\\n",
       "0                           1                        0   \n",
       "1                           0                        1   \n",
       "2                           0                        1   \n",
       "3                           0                        1   \n",
       "4                           2                        1   \n",
       "..                        ...                      ...   \n",
       "115                         1                        0   \n",
       "116                         1                        0   \n",
       "117                         1                        0   \n",
       "118                         1                        0   \n",
       "119                         0                        1   \n",
       "\n",
       "    NumberOfCorrectExamAnswers  Ratio LLM_Answer Exam_Answers  \\\n",
       "0                            1    1.0        [B]          [1]   \n",
       "1                            1    0.0        [B]          [3]   \n",
       "2                            1    0.0        [C]          [3]   \n",
       "3                            1    0.0        [B]          [0]   \n",
       "4                            2    1.0  [C, D, E]       [2, 4]   \n",
       "..                         ...    ...        ...          ...   \n",
       "115                          1    1.0        [A]          [0]   \n",
       "116                          1    1.0        [D]          [3]   \n",
       "117                          1    1.0        [B]          [1]   \n",
       "118                          1    1.0        [B]          [1]   \n",
       "119                          1    0.0        [D]          [1]   \n",
       "\n",
       "    Answered_Correctly Too_Many_answers  \n",
       "0                 True            False  \n",
       "1                False            False  \n",
       "2                False            False  \n",
       "3                False            False  \n",
       "4                False             True  \n",
       "..                 ...              ...  \n",
       "115               True            False  \n",
       "116               True            False  \n",
       "117               True            False  \n",
       "118               True            False  \n",
       "119              False            False  \n",
       "\n",
       "[120 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Questions</th>\n",
       "      <th>Correctly Answered</th>\n",
       "      <th>Incorrectly Answered</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Partial</th>\n",
       "      <th>Model</th>\n",
       "      <th>Total Partial Credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>61</td>\n",
       "      <td>59</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>70.416667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Number of Questions Correctly Answered Incorrectly Answered  Accuracy  \\\n",
       "0                 120                 61                   59  0.508333   \n",
       "\n",
       "   Accuracy Partial              Model  Total Partial Credit  \n",
       "0          0.586806  Phi-3-medium-128k             70.416667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.850902795791626 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy Mean</th>\n",
       "      <th>Accuracy Max</th>\n",
       "      <th>Accuracy Min</th>\n",
       "      <th>Accuracy STD</th>\n",
       "      <th>Accuracy Partial Mean</th>\n",
       "      <th>Accuracy Partial Max</th>\n",
       "      <th>Accuracy Partial Min</th>\n",
       "      <th>Accuracy Partial STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phi-3-medium-128k</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy Mean  Accuracy Max  Accuracy Min  Accuracy STD  \\\n",
       "0  Phi-3-medium-128k       0.508333      0.508333      0.508333           NaN   \n",
       "\n",
       "   Accuracy Partial Mean  Accuracy Partial Max  Accuracy Partial Min  \\\n",
       "0               0.586806              0.586806              0.586806   \n",
       "\n",
       "   Accuracy Partial STD  \n",
       "0                   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################\n",
    "#Main programm for the evaluation of the LLM\n",
    "###########################################\n",
    "\n",
    "valid_question_answer = False  \n",
    "#Create a dataframe with the size of NUM_OF_SHUFFLES which contains the dataframe llm_exam_result\n",
    "shuffled_evalutation_df = pd.DataFrame(columns=[ 'Number of Questions','Correctly Answered','Incorrectly Answered','Accuracy','Accuracy Partial'])\n",
    "\n",
    "#Read the questions from the questionsbank\n",
    "questions  = pd.read_parquet(QUESTIONS_BANK)\n",
    "\n",
    "#Randomly take NUMBER_OF_QUESTIONS (default 120)\n",
    "try:\n",
    "    questions = questions.sample(n=NUMBER_OF_QUESTIONS)\n",
    "except:\n",
    "    print(\"Number of questions is greater than the number of questions in the questionbank. Max Number taken\")\n",
    "\n",
    "#questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "#Iterate over each model definied in the MODEL_PATH dictionary\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "     #Load the model wiht LLamaCpp\n",
    "    print(\"Load Model ...\")\n",
    "    llm = LlamaCpp(\n",
    "        model_path= model_path,\n",
    "        n_gpu_layers=-1,\n",
    "        n_batch=512,\n",
    "        n_ctx=1100,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=1,\n",
    "        max_tokens = MAX_OUTPUT_TOKENS,\n",
    "        #callback_manager=callback_manager,\n",
    "        verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "    print(\"Model loaded\")\n",
    "    chain = prompt_template | llm\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.DataFrame(columns = [\"Model\", \"QuestionIndex\", \"SamplingIndex\", \"NumberOfChoices\", \"NumberOfCorrectLLMAnswers\", \"NumberOfIncorrectAnswers\", \"NumberOfCorrectExamAnswers\", \"Ratio\", \"LLM_Answer\", \"Exam_Answers\", \"Answered_Correctly\",  \"Too_Many_answers\"]) \n",
    "        #Iterate over each question in the question dataframe\n",
    "        #Start the timer\n",
    "        start_time = time.time()\n",
    "        for index_question, row in questions.iterrows():\n",
    "            question = row['question']\n",
    "            choices = row['choices']\n",
    "            answers = row['answer']\n",
    "            num_of_correct_answer = len(answers)\n",
    "\n",
    "            choices = format_choices_for_llm(choices)\n",
    "            num_of_choices = choices.count('\\n') + 1\n",
    "            #Only if shuffle is enabled, shuffle the choices\n",
    "            if shuffled_iteration > 0:\n",
    "                choices, answers = shuffle_choices_and_update_answer(row['choices'], row['answer'])\n",
    "                num_of_correct_answer = len(answers)\n",
    "                choices = format_choices_for_llm(choices)\n",
    "            #Empty the char_probabilities dictionary for each question\n",
    "            char_probabilities = {}\n",
    "\n",
    "            #Iterate over the maximum sampling rate\n",
    "            for index_sampling in range(MAX_SAMPLING_RATE):\n",
    "                # Invoke the chain with the question and choices              \n",
    "                llm_answer = chain.invoke({\"Exam_Question\" : row['question'], \"Exam_Choices\" : choices})     \n",
    "                # Check if the answer is in the expected format\n",
    "                if extract_answer(llm_answer) is not None:\n",
    "                    # Extract the correct answers from the LLM answer and analyse the answer\n",
    "                    num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly, num_of_incorrect_Answer = evaluation_sampling(llm_answer, answers, num_of_correct_answer)\n",
    "                    #Save the current sampling index -- How of the question has been asked until the answer was in the correct format\n",
    "                    sample_Index = index_sampling\n",
    "                    valid_question_answer = True\n",
    "                    break\n",
    "            \n",
    "            #Depending on the result of the answer, add the result to the dataframe\n",
    "            if not valid_question_answer:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [-1], \"NumberOfChoices\": num_of_choices, \"NumberOfCorrectLLMAnswers\": [0], \"NumberOfIncorrectAnswers\": num_of_incorrect_Answer, \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [-1], \"LLM_Answer\": [llm_answer], \"Exam_Answers\": [answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "            else:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [sample_Index],  \"NumberOfChoices\": num_of_choices, \"NumberOfIncorrectAnswers\": num_of_incorrect_Answer , \"NumberOfCorrectLLMAnswers\": [num_of_correct_llm_answer], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [num_of_correct_llm_answer/num_of_correct_answer], \"LLM_Answer\": [answerLLm], \"Exam_Answers\": [answers], \"Answered_Correctly\" : [answered_correctly], \"Too_Many_answers\": [too_many_answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "                valid_question_answer = False\n",
    "        answered_correctly = False\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            display(llm_exam_result)\n",
    "\n",
    "        if TRACK_RESULTS:\n",
    "            llm_exam_result.to_pickle(f\"{OUTPUT_PATH}{NUMBER_OF_QUESTIONS}_questions_{DATASET_NAME}_{model}_shuffled_{shuffled_iteration}.pkl\")\n",
    "       \n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            display(shuffled_evalutation_df)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            print(\"Time taken:\", elapsed_time, \"seconds\")\n",
    "\n",
    "\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "\n",
    "if PRINT_RESULTS:\n",
    "    display(model_statistics)\n",
    "\n",
    "if TRACK_RESULTS:\n",
    "    shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "    model_statistics.to_pickle(OUTPUT_EVALUATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Phi-3-medium-128k\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/120_questions_201-301-CCNA_Phi-3-medium-128k_shuffled_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shuffled_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_OF_SHUFFLES):\n\u001b[0;32m----> 7\u001b[0m     llm_exam_result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUMBER_OF_QUESTIONS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_questions_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATASET_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_shuffled_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mshuffled_iteration\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     evaluation_df \u001b[38;5;241m=\u001b[39m evaluation(llm_exam_result)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#Concat the evaluation dataframe to the complete dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/git/llm_test/.venv/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/llm_test/.venv/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/120_questions_5_Shot_HELM_201-301-CCNA_2024_07_21_19_45/120_questions_201-301-CCNA_Phi-3-medium-128k_shuffled_0.pkl'"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#Evaluation of the results and calculation of the statistics\n",
    "###########################################################\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.read_pickle(f\"{OUTPUT_PATH}{NUMBER_OF_QUESTIONS}_questions_{DATASET_NAME}_{model}_shuffled_{shuffled_iteration}.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "model_statistics.to_pickle(OUTPUT_EVALUATION)\n",
    "if \"mmlu\" in DATASET_NAME:\n",
    "    plot_evaluation_MMLU(model_statistics, HELM_RESULT, \"Own Approach\", \"HELM\", title=\"Own Approach vs. HELM Official\")\n",
    "else:\n",
    "    plot_evaluation_CCNA(model_statistics, hline_accuracy=0.5, hline_partial=0.5, title=DATASET_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows with new models and their accuracies\n",
    "new_rows = pd.DataFrame([\n",
    "    {'Model': 'Llama2-13b', 'Accuracy': 0.69},\n",
    "])\n",
    "\n",
    "HELM_RESULT = pd.concat([HELM_RESULT, new_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_path in MODEL_PATH.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for shuffled_iteration in range(1):\n",
    "        llm_exam_result = pd.read_pickle(f\"../results/120_questions_5_Shot_HELM_mmlu_Computer_Security_2024_07_14_20_08/{NUMBER_OF_QUESTIONS}_questions_mmlu_Computer_Security_{model}_shuffled_0.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.read_pickle(\"../results/120_questions_5_Shot_HELM_mmlu_Computer_Security_2024_07_14_20_08/llm_prob_result_detailed_mmlu_Computer_Security_5_Shot.pkl\")\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "plot_evaluation_MMLU(model_statistics, HELM_RESULT, \"Own Approach\", \"HELM\", title=\"Own Approach vs. HELM Official\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.read_pickle(f\"{OUTPUT_PATH}{NUMBER_OF_QUESTIONS}_questions_{DATASET_NAME}_{model}_shuffled_{shuffled_iteration}.pkl\")\n",
    "        #llm_exam_result = pd.read_pickle(f\"../data/{model}_shuffled_{shuffled_iteration}_201_301.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "model_statistics.to_pickle(OUTPUT_EVALUATION)\n",
    "plot_evaluation_CCNA(model_statistics, hline_accuracy=probability, hline_partial=probability, title=DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of MCQA with Cisco Certifications\n",
    "\n",
    "#Path to the main directory\n",
    "WORKSPACE_DIC = \"/hkfs/work/workspace_haic/scratch/sb7059-llm_models_jeremy\"\n",
    "\n",
    "\n",
    "#Dictionary with the model paths and the corresponding model names\n",
    "MODEL_PATH = { \"Mixtral-8x-7b\": WORKSPACE_DIC + \"/Mixtral/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "               \"Phi-2\": WORKSPACE_DIC + \"/Phi/Phi2/phi-2.Q4_K_M.gguf\",\n",
    "               \"Llama2-70b\": WORKSPACE_DIC + \"/Llama/Llama2/llama-2-70b.Q5_K_M.gguf\",\n",
    "                \"Phi-3-medium-128k\": WORKSPACE_DIC + \"/Phi/Phi3/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "               #\"LLama-3-70b\": WORKSPACE_DIC + \"/Llama/LLama3/Meta-Llama-3-70B-Instruct-v2.Q4_K_M.gguf\",\n",
    "               #\"Mixtral-8x22b\": WORKSPACE_DIC + \"/Mixtral/Mixtral-8x22b-Instruct\",\n",
    "               #\"Mixtral-8x-22b\": WORKSPACE_DIC + \"/Mixtral/Mixtral-8x22B-Instruct-v0.1.Q4_K_M-00001-of-00002.gguf\",\n",
    "              }\n",
    "\n",
    "########### Set the model parameters here ############\n",
    "\n",
    "#Parameter for changing the temperature of the model\n",
    "TEMPERATURE = 0\n",
    "#Parameter for max output tokens (for MMLU choose 1, since only Single choice)\n",
    "MAX_OUTPUT_TOKENS = 2\n",
    "\n",
    "######################################################\n",
    "\n",
    "#Sampling rate determines how often a question is asked again if the answer format is wrong\n",
    "MAX_SAMPLING_RATE = 5\n",
    "\n",
    "#Set to 1 if you dont want to shuffle\n",
    "NUM_OF_SHUFFLES = 1\n",
    "\n",
    "#Set the number of questions to be asked, default is 120 since it is the number of questions in a CCNA exam\n",
    "NUMBER_OF_QUESTIONS = 120\n",
    "\n",
    "########### Set the names for result / evaluation files here ############\n",
    "\n",
    "############# Turn of / off tracking of results and prints ############\n",
    "TRACK_RESULTS = True\n",
    "PRINT_RESULTS = True\n",
    "\n",
    "\n",
    "######################################################\n",
    "DATASET_NAME = \"201-301-CCNA\"\n",
    "DATASET_NAME = \"mmlu_Computer_Security\"\n",
    "DATE = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "#Set output path\n",
    "OUTPUT_PATH = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_HELM_{DATASET_NAME}_{DATE}/\"\n",
    "\n",
    "#Set output file name\n",
    "OUTPUT_EVALUATION = f\"{OUTPUT_PATH}llm_5_Shot_{DATASET_NAME}.pkl\"\n",
    "\n",
    "#Filename output evaluation detailed\n",
    "OUTPUT_EVALUATION_DETAILED = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_HELM_{DATASET_NAME}_{DATE}/llm_prob_result_detailed_{DATASET_NAME}_5_Shot.pkl\"\n",
    "\n",
    "#Set filename of json file\n",
    "OUTPUT_EVALUATION_JSON = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_HELM_{DATASET_NAME}_{DATE}/llm_prob_result_{DATASET_NAME}_5_Shot.json\"\n",
    "\n",
    "\n",
    "########### Set the names for result files here ############\n",
    "#Create Folder for results if not exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "########### Set the questionsbank here ############\n",
    "#Set the questionsbank\n",
    "QUESTIONS_BANK = \"../data/201-301-CCNA.parquet\" ##CCNA\n",
    "QUESTIONS_BANK = \"../data/mmlu_Computer_Security.parquet\" ##CCNA\n",
    "#QUESTIONS_BANK = \"../data/350-701-CCNP.parquet\" ##CCNP\n",
    "#QUESTIONS_BANK = \"../data/350-701-CCNP_no_image.parquet\" ##CCNP no Images\n",
    "HELM_RESULT = pd.read_pickle(\"../data/official_sec_mmlu_results.pkl\")\n",
    "########### Set the questionsbank here ############\n",
    "\n",
    "########### Set the prompt template here ############\n",
    "PROMPT_TEMPLATE = CCNA_5_SHOT_TEMPLATE_NO_WHITESPACE_AT_FINAL_ANW\n",
    "PROMPT_TEMPLATE = FEW_SHOT_TEMPLATE_MMLU_NO_WHITESPACE\n",
    "########### Set the prompt template here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for calculation of Probaibliity for CCNA 201-301 Exam ##\n",
    "# probabilities = []\n",
    "# for idx, row in CCNA_201_301.iterrows():\n",
    "#     total_choices = len(row['choices'])\n",
    "#     correct_answers = len(row['answer'])\n",
    "    \n",
    "#     # Wahrscheinlichkeit für eine richtige Antwort\n",
    "#     probability_one_correct = correct_answers / total_choices\n",
    "    \n",
    "#     # Wahrscheinlichkeit für alle richtige Antworten\n",
    "#     if correct_answers > 1:\n",
    "#         probability_all_correct = (correct_answers / total_choices) * ((correct_answers - 1) / (total_choices - 1))\n",
    "#     else:\n",
    "#         probability_all_correct = probability_one_correct\n",
    "\n",
    "#     probabilities.append(probability_all_correct)\n",
    "\n",
    "# # Sume the probabilities list and divide by the number of questions\n",
    "# probability = sum(probabilities) / len(probabilities)\n",
    "\n",
    "# probability\n",
    "probability = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
