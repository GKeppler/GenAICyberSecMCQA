{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from prompt_templates import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = {#\"Mixtral-8x-7b\": \"../models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "               \"Llama2-70b\": \"../models/llama2_70b_chat_uncensored.Q5_K_M.gguf\",\n",
    "               \"Dolphin-2.5\": \"../models/dolphin-2.5-mixtral-8x7b.Q5_K_M.gguf\",\n",
    "               \"Yi-34b\": \"../models/yi-34b-200k.Q5_K_M.gguf\",\n",
    "               \"Phi-2\": \"../models/phi-2.Q5_K_M.gguf\"\n",
    "              }\n",
    "\n",
    "#Set output file name\n",
    "OUTPUT_PICKEL = \"../data/llm_sampling_result_10.pkl\"\n",
    "\n",
    "MAX_SAMPLING_RATE = 10\n",
    "\n",
    "FEW_SHOT_TEMPLATE = \"\"\"\n",
    "    The following are multiple choice questions (with answers) about network fundamentals, network access,\n",
    "    security fundamentals, automation and programmability. Here is an example\n",
    "    \n",
    "    Question: Which two options are the best reasons to use an IPV4 private IP space? (Choose two.)\n",
    "    \n",
    "    Choices:\n",
    "    A. to enable intra-enterprise communication\n",
    "    B. to implement NAT\n",
    "    C. to connect applications\n",
    "    D. to conserve global address space\n",
    "    E. to manage routing overhead\n",
    "                                               \n",
    "    Correct Answer: ['A','D']\n",
    "    \n",
    "    Please give the answer in the following format:\n",
    "        Correct Answer: ['A','D']           \n",
    "    \n",
    "    Now, answer the following question:\n",
    "    Question: {Exam_Question}        \n",
    "                    \n",
    "    Choices:\n",
    "    {Exam_Choices}\n",
    "    \"\"\"\n",
    "\n",
    "PROBABILIY_ENABLED = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers (e.g., ['A', 'B']) if found, otherwise None. \n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r\"\"\"\n",
    "        ^\\s*                \n",
    "        Correct\\s+Answer:\\s+  \n",
    "        \\[                  \n",
    "        ['ABCDET\\s,]+      \n",
    "        \\]                  \n",
    "        \\s*                 \n",
    "    \"\"\"\n",
    "    match = re.search(pattern, answer, flags=re.VERBOSE)\n",
    "    if match:\n",
    "        # Extract the answer portion within the brackets\n",
    "        answer_section = match.group(0).split('[')[1].split(']')[0]\n",
    "\n",
    "        # Find individual answers (consider making this more robust if needed)\n",
    "        correct_answers = answer_section.strip().split(',')\n",
    "        return [ans.strip().strip(\"'\") for ans in correct_answers]\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compare_answers(answerLLM, answer_exam):\n",
    "    \"\"\"Compares the extracted correct answers with the answers in answer_exam.\n",
    "\n",
    "    Keyword arguments:\n",
    "    answerLLM -- the list of answers extracted from the LLM answer\n",
    "    answer_exam -- list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the answer_exam string to a list of answers\n",
    "    answer_exam_list = answer_exam.split(\" \")\n",
    "    #Get number of correct answers in the exam\n",
    "    num_of_correct_exam_answers = len(answer_exam_list)\n",
    "    \n",
    "\n",
    "    # Convert both lists to sets for efficient comparison\n",
    "    answer_LLM_set = set(answerLLM)\n",
    "    answer_exam_set = set(answer_exam_list)\n",
    "\n",
    "    # Calculate the count of matching answers\n",
    "    number_of_correct_llm_answers = len(answer_LLM_set.intersection(answer_exam_set))\n",
    "\n",
    "    # Check if the number of answers given by the LLM is greater than the number of correct answers\n",
    "    too_many_answ_given = False\n",
    "    if len(answer_LLM_set) > num_of_correct_exam_answers:\n",
    "        too_many_answ_given = True\n",
    "\n",
    "    # Return a dictionary with the matching count and the number of correct answers\n",
    "    return number_of_correct_llm_answers, too_many_answ_given\n",
    "\n",
    "def evaluation_sampling(llm_answer, exam_Answers, num_of_correct_answer):\n",
    "    \"\"\"Analyse the answer given by the LLM and compare it with the exam answers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    llm_answer -- the answer string given by the LLM\n",
    "    exam_Answers -- the list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    answerLLM = extract_answer(llm_answer)\n",
    "    if answerLLM is not None:\n",
    "        num_of_correct_llm_Answers, too_many_answ = compare_answers(answerLLM, exam_Answers)\n",
    "        if num_of_correct_llm_Answers == num_of_correct_answer and too_many_answ == False:\n",
    "            answered_correctly = True\n",
    "        else:\n",
    "            answered_correctly = False \n",
    "        return num_of_correct_llm_Answers, answerLLM, too_many_answ, answered_correctly\n",
    "    else:\n",
    "         return -1\n",
    "\n",
    "\n",
    "#####\n",
    "# Partial Credit allowed?\n",
    "####\n",
    "def evaluation(llm_output_dataframe):\n",
    "\n",
    "    # Compute the number of total questions for each model\n",
    "    number_of_questions = llm_output_dataframe.groupby('Model')['QuestionIndex'].count()\n",
    "    \n",
    "    #Number of fully correct answers given by the LLM\n",
    "    correctly_answered = llm_output_dataframe.groupby('Model')['Answered_Correctly'].sum()\n",
    "\n",
    "    #Number of incorrect answers given by the LLM\n",
    "    incorrectly_answered = number_of_questions - correctly_answered\n",
    "\n",
    "    #Amount of correct answers in the exam\n",
    "    amount_correcct_exam_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectExamAnswers'].sum()\n",
    "\n",
    "    #Amount of correct answers given by the LLM even if not fully correct\n",
    "    amount_correcct_llm_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectLLMAnswers'].sum()\n",
    "    \n",
    "    #Calculation of Precision and Recall and f1 score\n",
    "    precision = correctly_answered / number_of_questions\n",
    "    precision_partial = amount_correcct_llm_answers / amount_correcct_exam_answers\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Number of Questions': number_of_questions,\n",
    "        'Correctly Answered': correctly_answered,\n",
    "        'Incorrectly Answered': incorrectly_answered,\n",
    "        'Precision': precision,\n",
    "        'Precision Partial': precision_partial,\n",
    "    })\n",
    "\n",
    "    results_df = results_df.reset_index()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_evaluation(evaluation_df):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from a DataFrame containing columns:\n",
    "        - 'Model'\n",
    "        - 'Precision'\n",
    "        - 'Precision Partial'\n",
    "    \"\"\"\n",
    "    # --- Subplot 1: Precision and Recall ---\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    axs[0].bar(evaluation_df['Model'], evaluation_df['Precision'], bar_width, \n",
    "               label='Precision', color='#1f77b4')\n",
    "\n",
    "    # Add percentages\n",
    "    for i, row in evaluation_df.iterrows():\n",
    "        axs[0].text(row['Model'], row['Precision'] + 0.01, f\"{row['Precision']:.1%}\", \n",
    "                    ha='center', color='black')\n",
    "\n",
    "    axs[0].set_xlabel('Model')\n",
    "    axs[0].set_ylabel('Percentage of Questions')\n",
    "    axs[0].set_title('Precision and Recall', fontsize=12)\n",
    "    axs[0].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # --- Subplot 2: Partial Precision ---\n",
    "    axs[1].bar(evaluation_df['Model'], evaluation_df['Precision Partial'], bar_width,\n",
    "               label='Precision Partial', color='#2ca02c')\n",
    "\n",
    "    # Add percentages\n",
    "    for i, score in enumerate(evaluation_df['Precision Partial']):\n",
    "        axs[1].text(i + bar_width / 2, score + 0.01, f\"{score:.1%}\", ha='center', color='black')\n",
    "\n",
    "    axs[1].set_xlabel('Model')\n",
    "    axs[1].set_ylabel('Precision Partial')\n",
    "    axs[1].set_title('Precision Partial', fontsize=12)\n",
    "    axs[1].set_xticks([i + bar_width / 2 for i in range(len(evaluation_df))])\n",
    "    axs[1].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def extract_answer_from_text_file(path_to_text_file):\n",
    "   # Read the text file\n",
    "    with open(path_to_text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    questions = []\n",
    "    answers = []\n",
    "    correct_answers = []\n",
    "\n",
    "    # Parse the lines\n",
    "    question = \"\"\n",
    "    options = \"\"\n",
    "    correct_option = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"Question:\"):\n",
    "            if question:  # If it's not the first question\n",
    "                questions.append(question)\n",
    "                answers.append(options)\n",
    "                correct_answers.append(correct_option)\n",
    "                options = \"\"  # Reset options for the new question\n",
    "            question = line.replace(\"Question:\", \"\").strip()\n",
    "        elif line.startswith(\"Answer:\"):\n",
    "            correct_option = line.split(\":\")[-1].strip()\n",
    "        elif line.startswith((\"A.\", \"B.\", \"C.\", \"D.\", \"E.\")):\n",
    "            options += line.strip() + \" \"  # Add options to the string\n",
    "\n",
    "    # Append the last question\n",
    "    questions.append(question)\n",
    "    answers.append(options)\n",
    "    correct_answers.append(correct_option)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    data = {\n",
    "        'Question': questions,\n",
    "        'Answers': answers,\n",
    "        'Correct Answer': correct_answers\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_options_string(options_string):\n",
    "    # Use regular expression to split the string\n",
    "    options_list = re.split(r'(?=[A-Z]\\.)', options_string)\n",
    "    \n",
    "    # Remove any leading or trailing whitespace from each option\n",
    "    options_list = [option.strip() for option in options_list if option.strip()]\n",
    "    \n",
    "    return options_list\n",
    "\n",
    "def format_options_string(options_list):\n",
    "    # Join the options list with newline characters\n",
    "    formatted_options_string = '\\n'.join(options_list)\n",
    "    \n",
    "    return formatted_options_string\n",
    "\n",
    "\n",
    "def evaluation_probability(char_prob_list, llm_answer, exam_answers, num_of_correct_answer):\n",
    "    # Normalize probabilities\n",
    "    total = sum(char_prob_list.values())\n",
    "    for letter in char_prob_list:\n",
    "        char_prob_list[letter] /= total\n",
    "\n",
    "    # Sort characters by probability\n",
    "    sorted_chars = sorted(char_prob_list.items(), key=lambda item: item[1], reverse=True)\n",
    "    # Select answers based on the number of correct answers for the question\n",
    "    answerLLm = [char for char, prob in sorted_chars[:num_of_correct_answer]]\n",
    "    num_of_correct_llm_answer, too_many_answers = compare_answers(answerLLm, exam_answers)\n",
    "    \n",
    "    if num_of_correct_llm_answer == num_of_correct_answer and too_many_answers == False:\n",
    "        answered_correctly = True\n",
    "    else :\n",
    "        answered_correctly = False\n",
    "    return num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly\n",
    "\n",
    "def extract_exam_info(examQuestion):\n",
    "    #Extracting the question, answer and choices from the dataframe\n",
    "    question = examQuestion[0].strip()  # Get the value of the first column (question) and remove leading/trailing whitespace\n",
    "    answer_exam = examQuestion[2].strip()    # Get the value of the third column (answer) and remove leading/trailing whitespace\n",
    "    answer_exam_with_whitespace = ' '.join(list(answer_exam))\n",
    "\n",
    "    #Set the number of correct answers\n",
    "    num_of_correct_answer = len([char for char in answer_exam_with_whitespace if char != ' '])\n",
    "\n",
    "    #Create the choices string like in the prompt template\n",
    "    choices = \"\"  # Reset the choices variable for each question\n",
    "    choices = split_options_string(examQuestion[1])\n",
    "    choices = format_options_string(choices)\n",
    "\n",
    "    return question, choices, answer_exam_with_whitespace, num_of_correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "prompt_template = PromptTemplate.from_template(FEW_SHOT_TEMPLATE)\n",
    "\n",
    "\n",
    "\n",
    "# valid_question_answer = False  \n",
    "# for model, model_path in MODEL_PATH.items():\n",
    "#     #Load the model wiht LLamaCpp\n",
    "#     llm = LlamaCpp(\n",
    "#         model_path= model_path,\n",
    "#         n_gpu_layers=128,\n",
    "#         n_batch=512,\n",
    "#         #max_tokens = 100,\n",
    "#         #callback_manager=callback_manager,\n",
    "#         verbose=False,  # Verbose is required to pass to the callback manager\n",
    "#     )\n",
    "\n",
    "#     chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
      "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 46322.61 MiB on device 0: cudaMalloc failed: out of memory\n",
      "llama_model_load: error loading model: failed to allocate buffer\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: ../models/llama2_70b_chat_uncensored.Q5_K_M.gguf. Received error  (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Iterate over each model definied in the MODEL_PATH dictionary\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, model_path \u001b[38;5;129;01min\u001b[39;00m MODEL_PATH\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      9\u001b[0m      \u001b[38;5;66;03m#Load the model wiht LLamaCpp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#max_tokens = 100,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#callback_manager=callback_manager,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Verbose is required to pass to the callback manager\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     chain \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m#Iterate over each question in the question dataframe\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from path: ../models/llama2_70b_chat_uncensored.Q5_K_M.gguf. Received error  (type=value_error)"
     ]
    }
   ],
   "source": [
    "valid_question_answer = False  \n",
    "llm_exam_result = pd.DataFrame(columns = [\"Model\", \"QuestionIndex\", \"SamplingIndex\", \"NumberOfCorrectLLMAnswers\", \"NumberOfCorrectExamAnswers\", \"Ratio\", \"LLM_Answer\", \"Exam_Answers\", \"Answered_Correctly\",  \"Too_Many_answers\"]) \n",
    "questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "prompt_template = PromptTemplate.from_template(FEW_SHOT_TEMPLATE)\n",
    "\n",
    "#Iterate over each model definied in the MODEL_PATH dictionary\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "     #Load the model wiht LLamaCpp\n",
    "    llm = LlamaCpp(\n",
    "        model_path= model_path,\n",
    "        n_gpu_layers=128,\n",
    "        n_batch=512,\n",
    "        #max_tokens = 100,\n",
    "        #callback_manager=callback_manager,\n",
    "        verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "\n",
    "    #Iterate over each question in the question dataframe\n",
    "    for index_question, row in questions.iterrows():\n",
    "        #Extract the question, answer and choices from the dataframe\n",
    "        question, choices, answers, num_of_correct_answer = extract_exam_info(row)\n",
    "       \n",
    "        #Empty the char_probabilities dictionary for each question\n",
    "        char_probabilities = {}\n",
    "\n",
    "        #Iterate over the maximum sampling rate\n",
    "        for index_sampling in range(MAX_SAMPLING_RATE):\n",
    "\n",
    "            # Invoke the chain with the question and choices\n",
    "            llm_answer = chain.invoke({\"Exam_Question\" : question, \"Exam_Choices\" : choices})            \n",
    "\n",
    "            # Check if the answer is in the expected format\n",
    "            if extract_answer(llm_answer) is not None:\n",
    "\n",
    "                #If probability is enabled, check what the correct answer is after all the sampling\n",
    "                if not PROBABILIY_ENABLED:\n",
    "                    # Extract the correct answers from the LLM answer and analyse the answer\n",
    "                    num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly = evaluation_sampling(llm_answer, answers, num_of_correct_answer)\n",
    "                    #Save the current sampling index -- How of the question has been asked until the answer was in the correct format\n",
    "                    sample_Index = index_sampling\n",
    "                    valid_question_answer = True\n",
    "                    break\n",
    "                else:\n",
    "                    answer_letters = extract_answer(llm_answer)\n",
    "                    sample_Index = index_sampling\n",
    "                    valid_question_answer = True\n",
    "                    for letter in answer_letters:\n",
    "                        if letter in char_probabilities:\n",
    "                            char_probabilities[letter] += 1\n",
    "                        else:\n",
    "                            char_probabilities[letter] = 1\n",
    "        \n",
    "        if PROBABILIY_ENABLED:\n",
    "            num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly = evaluation_probability(char_probabilities, llm_answer, answers, num_of_correct_answer)\n",
    "    \n",
    "                       \n",
    "        #Depending on the result of the answer, add the result to the dataframe\n",
    "        if not valid_question_answer:\n",
    "            new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [-1], \"NumberOfCorrectLLMAnswers\": [0], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [-1], \"LLM_Answer\": [llm_answer], \"Exam_Answers\": [answers]})\n",
    "            llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "        else:\n",
    "            new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [sample_Index], \"NumberOfCorrectLLMAnswers\": [num_of_correct_llm_answer], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [num_of_correct_llm_answer/num_of_correct_answer], \"LLM_Answer\": [answerLLm], \"Exam_Answers\": [answers], \"Answered_Correctly\" : [answered_correctly], \"Too_Many_answers\": [too_many_answers]})\n",
    "            llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "            valid_question_answer = False\n",
    "    answered_correctly = False\n",
    "\n",
    "\n",
    "evaluation_df = evaluation(llm_exam_result)\n",
    "plot_evaluation(evaluation_df)\n",
    "display(evaluation_df)\n",
    "display(llm_exam_result)\n",
    "llm_exam_result.to_pickle(OUTPUT_PICKEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "\n",
    "llm_exam_result = pd.DataFrame(columns = [\"Model\", \"QuestionIndex\", \"SamplingIndex\", \"NumberOfCorrectLLMAnswers\", \"NumberOfCorrectExamAnswers\", \"Ratio\", \"LLM_Answer\", \"Exam_Answers\", \"Answered_Correctly\",  \"Too_Many_answers\"]) \n",
    "valid_question_answer = False  \n",
    "for model, model_path in MODEL_PATH.items():\n",
    "    #Load the model wiht LLamaCpp\n",
    "    llm = LlamaCpp(\n",
    "        model_path= model_path,\n",
    "        n_gpu_layers=128,\n",
    "        n_batch=512,\n",
    "        #max_tokens = 100,\n",
    "        #callback_manager=callback_manager,\n",
    "        verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "\n",
    "    chain = prompt_template | llm\n",
    "\n",
    "\n",
    "    for index_question, row in questions.iterrows():\n",
    "        question = row[0].strip()  # Get the value of the first column (question) and remove leading/trailing whitespace\n",
    "        options = row[1].split('\\n')  # Split the options by newline character\n",
    "        options = [opt.strip() for opt in options]  # Strip leading/trailing whitespace from each option\n",
    "        answer_exam = row[2].strip()    # Get the value of the third column (answer) and remove leading/trailing whitespace\n",
    "        answer_exam_with_whitespace = ' '.join(list(answer_exam))\n",
    "        options = list(filter(None, options))\n",
    "\n",
    "        choices = \"\"  # Reset the choices variable for each question\n",
    "\n",
    "        # Create the choices string\n",
    "        for i, opt in enumerate(options):\n",
    "            choices += f\"{opt}\" \n",
    "\n",
    "        for index_sampling in range(MAX_SAMPLING_RATE):\n",
    "            llm_answer = chain.invoke({\"Exam_Question\" : question, \"Exam_Choices\" : choices})\n",
    "\n",
    "            # Check if the answer is in the expected format\n",
    "            if is_valid_answer(llm_answer):\n",
    "\n",
    "                # Extract the correct answers from the LLM answer and analyse the answer\n",
    "                num_of_correct_llm_answer, num_of_correct_answer, answerLLm, too_many_answers = analyse_answer(llm_answer, answer_exam_with_whitespace)\n",
    "                sample_Index = index_sampling\n",
    "                valid_question_answer = True\n",
    "\n",
    "                # Check if the number of correct answers given by the LLM is equal to the number of correct answers\n",
    "                if num_of_correct_llm_answer == num_of_correct_answer and too_many_answers == False:\n",
    "                    answered_correctly = True\n",
    "                else :\n",
    "                    answered_correctly = False\n",
    "                break\n",
    "        if not valid_question_answer:\n",
    "            llm_exam_result = llm_exam_result.append({\"Model\": model, \"QuestionIndex\": index_question, \"SamplingIndex\": -1, \"NumberOfCorrectLLMAnswers\": -1, \"NumberOfCorrectExamAnswers\": -1, \"Ratio\": -1, \"LLM_Answer\": llm_answer, \"Exam_Answers\": answer_exam_with_whitespace}, ignore_index=True)\n",
    "        else:\n",
    "            llm_exam_result = llm_exam_result.append({\"Model\": model, \"QuestionIndex\": index_question, \"SamplingIndex\": sample_Index, \"NumberOfCorrectLLMAnswers\": num_of_correct_llm_answer, \"NumberOfCorrectExamAnswers\": num_of_correct_answer, \"Ratio\": num_of_correct_llm_answer/num_of_correct_answer, \"LLM_Answer\": answerLLm, \"Exam_Answers\": answer_exam_with_whitespace, \"Answered_Correctly\" : answered_correctly, \"Too_Many_answers\": too_many_answers}, ignore_index=True)\n",
    "            valid_question_answer = False\n",
    "            answered_correctly = False\n",
    "\n",
    "    display(llm_exam_result)\n",
    "    evaluation_df = evaluation(llm_exam_result)\n",
    "    #plot_evaluation(evaluation_df)\n",
    "    display(evaluation_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
