{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates import *\n",
    "import time\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "#Set the output limit to inf\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only for calculation of Probaibliity for CCNA 201-301 Exam ##\n",
    "# probabilities = []\n",
    "# for idx, row in CCNA_201_301.iterrows():\n",
    "#     total_choices = len(row['choices'])\n",
    "#     correct_answers = len(row['answer'])\n",
    "    \n",
    "#     # Wahrscheinlichkeit für eine richtige Antwort\n",
    "#     probability_one_correct = correct_answers / total_choices\n",
    "    \n",
    "#     # Wahrscheinlichkeit für alle richtige Antworten\n",
    "#     if correct_answers > 1:\n",
    "#         probability_all_correct = (correct_answers / total_choices) * ((correct_answers - 1) / (total_choices - 1))\n",
    "#     else:\n",
    "#         probability_all_correct = probability_one_correct\n",
    "\n",
    "#     probabilities.append(probability_all_correct)\n",
    "\n",
    "# # Sume the probabilities list and divide by the number of questions\n",
    "# probability = sum(probabilities) / len(probabilities)\n",
    "\n",
    "# probability\n",
    "probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_DIC = \"/hkfs/work/workspace_haic/scratch/sb7059-llm_models_jeremy\"\n",
    "\n",
    "MODEL_PATH = { \"Mixtral-8x-7b\":\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "               \"Phi-2\":\"microsoft/phi-2\",\n",
    "               \"Llama2-70b\":\"meta-llama/Llama-2-70b-chat-hf\",\n",
    "               \"Yi-34b\":\"01-ai/Yi-1.5-34B-Chat\",\n",
    "               #\"Llama-65b\": WORKSPACE_DIC + \"/llama-65b.Q5_K_M.gguf\",\n",
    "               #\"LLama-3-70b\": WORKSPACE_DIC + \"/Meta-Llama-3-70B-Instruct-v2.Q4_K_M.gguf\",\n",
    "               #\"Phi-3-medium-128k\": WORKSPACE_DIC + \"/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "               #\"Mixtral-8x22b\": WORKSPACE_DIC + \"/mixtral-8x22b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "              }\n",
    "\n",
    "########### Set the model parameters here ############\n",
    "\n",
    "#Parameter for changing the temperature of the model\n",
    "TEMPERATURE = 0\n",
    "#Parameter for max output tokens (for MMLU choose 1, since only Single choice)\n",
    "MAX_OUTPUT_TOKENS = 2\n",
    "\n",
    "######################################################\n",
    "\n",
    "#Sampling rate determines how often a question is asked again if the answer format is wrong\n",
    "MAX_SAMPLING_RATE = 5\n",
    "\n",
    "#Set to 1 if you dont want to shuffle\n",
    "NUM_OF_SHUFFLES = 1\n",
    "\n",
    "NUMBER_OF_QUESTIONS = 117\n",
    "\n",
    "########### Set the names for result / evaluation files here ############\n",
    "\n",
    "############# Turn of / off tracking of results and prints ############\n",
    "TRACK_RESULTS = True\n",
    "PRINT_RESULTS = True\n",
    "######################################################\n",
    "DATASET_NAME = \"350-701-CCNP\"\n",
    "#Set output path\n",
    "OUTPUT_PATH = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_{DATASET_NAME}/\"\n",
    "\n",
    "#Set output file name\n",
    "OUTPUT_EVALUATION = f\"{OUTPUT_PATH}llm_5_Shot_{DATASET_NAME}.pkl\"\n",
    "\n",
    "#Filename output evaluation detailed\n",
    "OUTPUT_EVALUATION_DETAILED = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_{DATASET_NAME}/llm_prob_result_detailed_{DATASET_NAME}_5_Shot.pkl\"\n",
    "\n",
    "#Set filename of json file\n",
    "OUTPUT_EVALUATION_JSON = f\"../results/{NUMBER_OF_QUESTIONS}_questions_5_Shot_{DATASET_NAME}/llm_prob_result_{DATASET_NAME}_5_Shot.json\"\n",
    "\n",
    "\n",
    "########### Set the names for result files here ############\n",
    "#Create Folder for results if not exists\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "\n",
    "########### Set the questionsbank here ############\n",
    "#Set the questionsbank\n",
    "#QUESTIONS_BANK = \"../data/201-301-CCNA.parquet\" ##CCNA\n",
    "#QUESTIONS_BANK = \"../data/mmlu_Computer_Security.parquet\" ##CCNA\n",
    "#QUESTIONS_BANK = \"../data/350-701-CCNP.parquet\" ##CCNP\n",
    "QUESTIONS_BANK = \"../../data/350-701-CCNP_no_image.parquet\" ##CCNP no Images\n",
    "HELM_RESULT = pd.read_pickle(\"../../data/official_sec_mmlu_results.pkl\")\n",
    "########### Set the questionsbank here ############\n",
    "\n",
    "########### Set the prompt template here ############\n",
    "PROMPT_TEMPLATE = CCNA_5_SHOT_TEMPLATE_NO_WHITESPACE_AT_FINAL_ANW\n",
    "########### Set the prompt template here ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save the parameters as a JSON file\n",
    "if TRACK_RESULTS:\n",
    "    parameters = {\n",
    "        \"RUN_NAME\": DATASET_NAME,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"QUESTION_BANK\": QUESTIONS_BANK,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"MAX_SAMPLING_RATE\": MAX_SAMPLING_RATE,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"NUM_OF_SHUFFLES\": NUM_OF_SHUFFLES,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"FEW_SHOT_TEMPLATE\": PROMPT_TEMPLATE,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"TEMPERATURE\": TEMPERATURE,  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "        \"MAX_TOKENS\": MAX_OUTPUT_TOKENS  # Ersetzen Sie durch den tatsächlichen Wert\n",
    "    }\n",
    "\n",
    "    # Speichern Sie das Wörterbuch als JSON-Datei\n",
    "    with open(OUTPUT_EVALUATION_JSON, 'w') as f:\n",
    "        json.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(answer):\n",
    "    \"\"\"Extracts the correct answers from the provided answer string.\n",
    "\n",
    "    Args:\n",
    "        answer: The answer string to extract the correct answers from.\n",
    "\n",
    "    Returns:\n",
    "        A list of correct answers (e.g., ['A', 'B']) if found, otherwise None. \n",
    "    \"\"\"\n",
    "    #print(repr(answer))\n",
    "    answer = re.sub(r'[\\s\\n.,]', '', answer)\n",
    "    pattern = re.compile(r'^[A-Z,]*$')\n",
    "    #print(answer)\n",
    "    if re.match(pattern, answer):\n",
    "        if ',' in answer:\n",
    "            return None\n",
    "        else:\n",
    "            return list(answer)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def compare_answers(answerLLM, answer_exam):\n",
    "    \"\"\"Compares the extracted correct answers with the answers in answer_exam.\n",
    "\n",
    "    Keyword arguments:\n",
    "    answerLLM -- the list of answers extracted from the LLM answer\n",
    "    answer_exam -- list of answers from the exam\n",
    "    \"\"\"\n",
    "    # Convert answer_exam_list from letters to numbers\n",
    "    answerLLM = [ord(answer) - 65 for answer in answerLLM]\n",
    "\n",
    "    # Get number of correct answers in the exam\n",
    "    num_of_correct_exam_answers = len(answer_exam)\n",
    "\n",
    "    # Convert both lists to sets for efficient comparison\n",
    "    answer_LLM_set = set(answerLLM)\n",
    "    answer_exam_set = set(answer_exam)\n",
    "\n",
    "    # Calculate the count of matching answers\n",
    "    number_of_correct_llm_answers = len(answer_LLM_set.intersection(answer_exam_set))\n",
    "\n",
    "    # Check if the number of answers given by the LLM is greater than the number of correct answers\n",
    "    too_many_answ_given = False\n",
    "    if len(answer_LLM_set) > num_of_correct_exam_answers:\n",
    "        too_many_answ_given = True\n",
    "\n",
    "    # Return a dictionary with the matching count and the number of correct answers\n",
    "    return number_of_correct_llm_answers, too_many_answ_given\n",
    "\n",
    "def format_choices_for_llm(choices):\n",
    "    #Define the letters for the choices\n",
    "    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    \n",
    "    # Erstellen Sie den formatierten String\n",
    "    formatted_choices = '\\n'.join(f'{letters[i]}. {choice}' for i, choice in enumerate(choices))\n",
    "    \n",
    "    return formatted_choices\n",
    "\n",
    "def evaluation_sampling(llm_answer, exam_Answers, num_of_correct_answer):\n",
    "    \"\"\"Analyse the answer given by the LLM and compare it with the exam answers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    llm_answer -- the answer string given by the LLM\n",
    "    exam_Answers -- the list of answers from the exam\n",
    "    \"\"\"\n",
    "\n",
    "    answerLLM = extract_answer(llm_answer)\n",
    "    if answerLLM is not None:\n",
    "        num_of_correct_llm_Answers, too_many_answ = compare_answers(answerLLM, exam_Answers)\n",
    "        if num_of_correct_llm_Answers == num_of_correct_answer and too_many_answ == False:\n",
    "            answered_correctly = True\n",
    "        else:\n",
    "            answered_correctly = False \n",
    "        return num_of_correct_llm_Answers, answerLLM, too_many_answ, answered_correctly\n",
    "    else:\n",
    "         return -1\n",
    "\n",
    "\n",
    "def evaluation(llm_output_dataframe):\n",
    "\n",
    "    # Compute the number of total questions for each model\n",
    "    number_of_questions = llm_output_dataframe.groupby('Model')['QuestionIndex'].count()\n",
    "    \n",
    "    #Number of fully correct answers given by the LLM\n",
    "    correctly_answered = llm_output_dataframe.groupby('Model')['Answered_Correctly'].sum()\n",
    "\n",
    "    #Number of incorrect answers given by the LLM\n",
    "    incorrectly_answered = number_of_questions - correctly_answered\n",
    "\n",
    "    #Amount of correct answers in the exam\n",
    "    amount_correcct_exam_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectExamAnswers'].sum()\n",
    "\n",
    "    #Amount of correct answers given by the LLM even if not fully correct\n",
    "    amount_correcct_llm_answers = llm_output_dataframe.groupby('Model')['NumberOfCorrectLLMAnswers'].sum()\n",
    "    \n",
    "    #Calculation of Accuracy and Recall and f1 score\n",
    "    accuracy = correctly_answered / number_of_questions\n",
    "    accuracy_partial = amount_correcct_llm_answers / amount_correcct_exam_answers\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame({\n",
    "        'Number of Questions': number_of_questions,\n",
    "        'Correctly Answered': correctly_answered,\n",
    "        'Incorrectly Answered': incorrectly_answered,\n",
    "        'Accuracy': accuracy,\n",
    "        'Accuracy Partial': accuracy_partial,\n",
    "    })\n",
    "\n",
    "    results_df = results_df.reset_index()\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def plot_evaluation_CCNA(evaluation_df, hline_accuracy=None, hline_partial=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from a DataFrame containing columns:\n",
    "        - 'Model'\n",
    "        - 'Accuracy Mean', 'Accuracy Min', 'Accuracy Max'\n",
    "        - 'Accuracy Partial Mean', 'Accuracy Partial Min', 'Accuracy Partial Max'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a list of colors for the models\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "    # Define bar width\n",
    "    bar_width = 0.5  # Increase bar width for thicker bars\n",
    "\n",
    "    # --- Subplot 1: Accuracy ---\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[0].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Mean'], bar_width, \n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Mean'] - evaluation_df.loc[i, 'Accuracy Min'])], [abs(evaluation_df.loc[i, 'Accuracy Max'] - evaluation_df.loc[i, 'Accuracy Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[0].set_ylabel('Accuracy (%)')\n",
    "    axs[0].set_title('Accuracy Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[0].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[0].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylim([0, 1])\n",
    "    axs[0].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[0].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Accuracy subplot\n",
    "    if hline_accuracy is not None:\n",
    "        axs[0].axhline(y=hline_accuracy, color='r', linestyle='--')\n",
    "\n",
    "    # --- Subplot 2: Partial Accuracy ---\n",
    "    for i, model in enumerate(evaluation_df['Model']):\n",
    "        bars = axs[1].bar(i + bar_width * i, evaluation_df.loc[i, 'Accuracy Partial Mean'], bar_width,\n",
    "                   yerr=[[abs(evaluation_df.loc[i, 'Accuracy Partial Mean'] - evaluation_df.loc[i, 'Accuracy Partial Min'])], [abs(evaluation_df.loc[i, 'Accuracy Partial Max'] - evaluation_df.loc[i, 'Accuracy Partial Mean'])]],\n",
    "                   label=model, color=colors[i % len(colors)], capsize=5)\n",
    "\n",
    "    axs[1].set_ylabel('Accuracy Partial (%)')\n",
    "    axs[1].set_title('Accuracy Partial Mean with Error Bars (Max and Min)', fontsize=12)\n",
    "    axs[1].set_xticks([i + bar_width * i for i in range(len(evaluation_df['Model']))])\n",
    "    axs[1].set_xticklabels(evaluation_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    axs[1].legend()\n",
    "    axs[1].set_ylim([0, 1])\n",
    "    axs[1].yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    axs[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axs[1].grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Partial Accuracy subplot\n",
    "    if hline_partial is not None:\n",
    "        axs[1].axhline(y=hline_partial, color='r', linestyle='--')\n",
    "\n",
    "    fig.tight_layout(pad=1.2)  # Decrease padding for closer plots\n",
    "\n",
    "    # Add title to the figure\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16, y=1.05)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def calculate_model_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculates statistics for each model in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    df (DataFrame): Input DataFrame containing evaluation metrics for different models.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: New DataFrame containing calculated statistics for each model.\n",
    "    \"\"\"\n",
    "    model_stats = []\n",
    "    for model, group_df in df.groupby('Model'):\n",
    "        model_stat = {\n",
    "            'Model': model,\n",
    "            'Accuracy Mean': group_df['Accuracy'].mean(),\n",
    "            'Accuracy Max': group_df['Accuracy'].max(),\n",
    "            'Accuracy Min': group_df['Accuracy'].min(),\n",
    "            'Accuracy STD': group_df['Accuracy'].std(),\n",
    "            'Accuracy Partial Mean': group_df['Accuracy Partial'].mean(),\n",
    "            'Accuracy Partial Max': group_df['Accuracy Partial'].max(),\n",
    "            'Accuracy Partial Min': group_df['Accuracy Partial'].min(),\n",
    "            'Accuracy Partial STD': group_df['Accuracy Partial'].std()\n",
    "        }\n",
    "        model_stats.append(model_stat)\n",
    "    \n",
    "    return pd.DataFrame(model_stats)\n",
    "\n",
    "\n",
    "def shuffle_choices_and_update_answer(choices, answer):\n",
    "    # Erstellen Sie eine Liste von Indizes und mischen Sie sie\n",
    "    indices = list(range(len(choices)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    # Verwenden Sie die gemischten Indizes, um die Auswahlmöglichkeiten und die Antwort zu aktualisieren\n",
    "    shuffled_choices = [choices[i] for i in indices]\n",
    "    updated_answer = [indices.index(a) for a in answer]  # Entfernen Sie +1, um 0-basierte Indizes zu verwenden\n",
    "    \n",
    "    return shuffled_choices, updated_answer\n",
    "\n",
    "def plot_evaluation_MMLU(llm_result_df, helm_result, df1_name, df2_name, title=None):\n",
    "    \"\"\"\n",
    "    Plots evaluation metrics from two DataFrames containing columns:\n",
    "        - 'Model'\n",
    "        - 'Accuracy'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define colors for the models\n",
    "    color_llm_result = 'b'\n",
    "    color_helm_result = 'orange'\n",
    "\n",
    "    # Define bar width and gap\n",
    "    bar_width = 0.2  # Decrease bar width for side-by-side bars with a gap\n",
    "    gap = 0.05  # Define gap between bars\n",
    "\n",
    "    # Merge the two dataframes by column Model and create a new dataframe and rename the column Accuracy Mean to Accuracy Mean LLM and Accuracy to Accuracy HELM\n",
    "    llm_result_df = pd.merge(llm_result_df, helm_result, on='Model', suffixes=('_LLM', '_HELM'))\n",
    "    llm_result_df = llm_result_df.rename(columns={'Accuracy': 'Accuracy_HELM', 'Accuracy Mean': 'Accuracy_Mean_LLM'})\n",
    "\n",
    "    display(llm_result_df)\n",
    "\n",
    "    # Just one plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    for i, model in enumerate(llm_result_df['Model']):\n",
    "        bars_llm = ax.bar(i - bar_width - gap / 2, llm_result_df.loc[i, 'Accuracy_Mean_LLM'], bar_width, color=color_llm_result)\n",
    "        bars_helm = ax.bar(i + gap / 2, llm_result_df.loc[i, 'Accuracy_HELM'], bar_width, color=color_helm_result)\n",
    "\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('Accuracy Mean LLM vs. HELM', fontsize=12)\n",
    "    ax.set_xticks([i for i in range(len(llm_result_df['Model']))])\n",
    "    ax.set_xticklabels(llm_result_df['Model'], rotation=45, ha='right', fontsize=10)\n",
    "    ax.legend([bars_llm, bars_helm], [df1_name, df2_name])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.yaxis.set_major_locator(mtick.MultipleLocator(0.1))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.grid(True, linestyle='dotted', axis='y')\n",
    "\n",
    "    # Add horizontal line to Accuracy subplot\n",
    "    ax.axhline(y=probability, color='r', linestyle='--')\n",
    "\n",
    "    # Add title to the figure\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16, y=1.05)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 17:40:20,957\tINFO worker.py:1582 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Iterate over each model definied in the MODEL_PATH dictionary\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, model_path \u001b[38;5;129;01min\u001b[39;00m MODEL_PATH\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#Load the model wiht vLLM\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     llm \u001b[38;5;241m=\u001b[39m \u001b[43mVLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMAX_OUTPUT_TOKENS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# mandatory for hf models\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#callback_manager=callback_manager,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Verbose is required to pass to the callback manager\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     chain \u001b[38;5;241m=\u001b[39m prompt_template \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shuffled_iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_OF_SHUFFLES):\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/pydantic/v1/main.py:1100\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1100\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1102\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/langchain_community/llms/vllm.py:88\u001b[0m, in \u001b[0;36mVLLM.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import vllm python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install vllm`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mVLLModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtensor_parallel_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdownload_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvllm_kwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/vllm/entrypoints/llm.py:123\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    103\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    104\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    105\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    122\u001b[0m )\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/vllm/engine/llm_engine.py:282\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    280\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m CPUExecutor\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mworker_use_ray:\n\u001b[0;32m--> 282\u001b[0m     \u001b[43minitialize_ray_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mray_gpu_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RayGPUExecutor\n\u001b[1;32m    284\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m RayGPUExecutor\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/vllm/executor/ray_utils.py:116\u001b[0m, in \u001b[0;36minitialize_ray_cluster\u001b[0;34m(parallel_config, ray_address)\u001b[0m\n\u001b[1;32m    111\u001b[0m     current_placement_group \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mplacement_group(\n\u001b[1;32m    112\u001b[0m         placement_group_specs)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Wait until PG is ready - this will block until all\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# requested resources are available, and will timeout\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# if they cannot be provisioned.\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_placement_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Set the placement group in the parallel config\u001b[39;00m\n\u001b[1;32m    119\u001b[0m parallel_config\u001b[38;5;241m.\u001b[39mplacement_group \u001b[38;5;241m=\u001b[39m current_placement_group\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/ray/_private/worker.py:2623\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2618\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/git/llm_test/vLLM_Test/.venvVLLM/lib/python3.10/site-packages/ray/_private/worker.py:840\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    835\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m         )\n\u001b[1;32m    839\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 840\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3485\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:571\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_question_answer = False  \n",
    "#Create a dataframe with the size of NUM_OF_SHUFFLES which contains the dataframe llm_exam_result\n",
    "shuffled_evalutation_df = pd.DataFrame(columns=[ 'Number of Questions','Correctly Answered','Incorrectly Answered','Accuracy','Accuracy Partial'])\n",
    "\n",
    "questions  = pd.read_parquet(QUESTIONS_BANK)\n",
    "\n",
    "#Randomly take 120 questions\n",
    "questions = questions.sample(n=NUMBER_OF_QUESTIONS)\n",
    "\n",
    "#Take the first 20 questions\n",
    "#questions = questions.head(20)\n",
    "\n",
    "#questions = extract_answer_from_text_file(\"../data/questionbank_cisco_CCNP.txt\")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "prompt_template = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "#Iterate over each model definied in the MODEL_PATH dictionary\n",
    "for model, model_path in MODEL_PATH.items():\n",
    "    #Load the model wiht vLLM\n",
    "    llm = VLLM(\n",
    "    model=model_path,\n",
    "    n_gpu_layers=128,\n",
    "    tensor_parallel_size=2,\n",
    "    n_batch=256,\n",
    "    n_ctx=256,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=1,\n",
    "    max_new_tokens = MAX_OUTPUT_TOKENS,\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "    chain = prompt_template | llm\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.DataFrame(columns = [\"Model\", \"QuestionIndex\", \"SamplingIndex\", \"NumberOfCorrectLLMAnswers\", \"NumberOfCorrectExamAnswers\", \"Ratio\", \"LLM_Answer\", \"Exam_Answers\", \"Answered_Correctly\",  \"Too_Many_answers\"]) \n",
    "        #Iterate over each question in the question dataframe\n",
    "        \n",
    "        #Start the timer\n",
    "        start_time = time.time()\n",
    "        for index_question, row in questions.iterrows():\n",
    "            question = row['question']\n",
    "            choices = row['choices']\n",
    "            answers = row['answer']\n",
    "            num_of_correct_answer = len(answers)\n",
    "\n",
    "            choices = format_choices_for_llm(choices)\n",
    "\n",
    "            #Only if shuffle is enabled, shuffle the choices\n",
    "            if shuffled_iteration > 0:\n",
    "                choices, answers = shuffle_choices_and_update_answer(row['choices'], row['answer'])\n",
    "                num_of_correct_answer = len(answers)\n",
    "                choices = format_choices_for_llm(choices)\n",
    "            #Empty the char_probabilities dictionary for each question\n",
    "            char_probabilities = {}\n",
    "\n",
    "            #Iterate over the maximum sampling rate\n",
    "            for index_sampling in range(MAX_SAMPLING_RATE):\n",
    "                # Invoke the chain with the question and choices              \n",
    "                \n",
    "\n",
    "                ########### Print the question and choices ############\n",
    "                #print(f\"Question: {question}\")\n",
    "                #print(choices)\n",
    "\n",
    "                llm_answer = chain.invoke({\"Exam_Question\" : row['question'], \"Exam_Choices\" : choices})     \n",
    "                #print(llm_answer)       \n",
    "                # Check if the answer is in the expected format\n",
    "                if extract_answer(llm_answer) is not None:\n",
    "                    # Extract the correct answers from the LLM answer and analyse the answer\n",
    "                    num_of_correct_llm_answer, answerLLm, too_many_answers, answered_correctly = evaluation_sampling(llm_answer, answers, num_of_correct_answer)\n",
    "                    #Save the current sampling index -- How of the question has been asked until the answer was in the correct format\n",
    "                    sample_Index = index_sampling\n",
    "                    valid_question_answer = True\n",
    "                    break\n",
    "            \n",
    "            #Depending on the result of the answer, add the result to the dataframe\n",
    "            if not valid_question_answer:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [-1], \"NumberOfCorrectLLMAnswers\": [0], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [-1], \"LLM_Answer\": [llm_answer], \"Exam_Answers\": [answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "            else:\n",
    "                new_row = pd.DataFrame({\"Model\": [model], \"QuestionIndex\": [index_question], \"SamplingIndex\": [sample_Index], \"NumberOfCorrectLLMAnswers\": [num_of_correct_llm_answer], \"NumberOfCorrectExamAnswers\": [num_of_correct_answer], \"Ratio\": [num_of_correct_llm_answer/num_of_correct_answer], \"LLM_Answer\": [answerLLm], \"Exam_Answers\": [answers], \"Answered_Correctly\" : [answered_correctly], \"Too_Many_answers\": [too_many_answers]})\n",
    "                llm_exam_result = pd.concat([llm_exam_result, new_row], ignore_index=True)\n",
    "                valid_question_answer = False\n",
    "        answered_correctly = False\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            display(llm_exam_result)\n",
    "\n",
    "        if TRACK_RESULTS:\n",
    "            llm_exam_result.to_pickle(f\"{OUTPUT_PATH}{NUMBER_OF_QUESTIONS}_questions_{DATASET_NAME}_{model}_shuffled_{shuffled_iteration}.pkl\")\n",
    "       \n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            display(shuffled_evalutation_df)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "\n",
    "        if PRINT_RESULTS:\n",
    "            print(\"Time taken:\", elapsed_time, \"seconds\")\n",
    "\n",
    "\n",
    "#plot_evaluation(shuffled_evalutation_df)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "\n",
    "if PRINT_RESULTS:\n",
    "    display(model_statistics)\n",
    "\n",
    "#plot_evaluation_MMLU(model_statistics, HELM_RESULT, \"Own Approach\", \"HELM\", title=\"Own Approach vs. HELM Official\")\n",
    "if TRACK_RESULTS:\n",
    "    shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "    model_statistics.to_pickle(OUTPUT_EVALUATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_path in MODEL_PATH.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for shuffled_iteration in range(NUM_OF_SHUFFLES):\n",
    "        llm_exam_result = pd.read_pickle(f\"{OUTPUT_PATH}{NUMBER_OF_QUESTIONS}_questions_{DATASET_NAME}_{model}_shuffled_{shuffled_iteration}.pkl\")\n",
    "        #llm_exam_result = pd.read_pickle(f\"../data/{model}_shuffled_{shuffled_iteration}_201_301.pkl\")\n",
    "        evaluation_df = evaluation(llm_exam_result)\n",
    "        #Concat the evaluation dataframe to the complete dataframe\n",
    "        shuffled_evalutation_df = pd.concat([shuffled_evalutation_df, evaluation_df], ignore_index=True)\n",
    "model_statistics = calculate_model_statistics(shuffled_evalutation_df)\n",
    "display(model_statistics)\n",
    "shuffled_evalutation_df.to_pickle(OUTPUT_EVALUATION_DETAILED)\n",
    "model_statistics.to_pickle(OUTPUT_EVALUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_CCNA(model_statistics, hline_accuracy=probability, hline_partial=probability+0.1, title=DATASET_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
